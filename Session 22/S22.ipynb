{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T02:19:15.673388Z",
     "iopub.status.busy": "2023-11-17T02:19:15.673073Z",
     "iopub.status.idle": "2023-11-17T02:19:30.040115Z",
     "shell.execute_reply": "2023-11-17T02:19:30.039030Z",
     "shell.execute_reply.started": "2023-11-17T02:19:15.673361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.12.2)\n",
      "Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.7.22)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Installing collected packages: gdown\n",
      "Successfully installed gdown-4.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T02:23:02.042243Z",
     "iopub.status.busy": "2023-11-17T02:23:02.041833Z",
     "iopub.status.idle": "2023-11-17T02:23:23.323457Z",
     "shell.execute_reply": "2023-11-17T02:23:23.322200Z",
     "shell.execute_reply.started": "2023-11-17T02:23:02.042203Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub sentencepiece tokenizers lightning fabric jsonargparse -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T02:23:23.327639Z",
     "iopub.status.busy": "2023-11-17T02:23:23.327302Z",
     "iopub.status.idle": "2023-11-17T02:25:38.441454Z",
     "shell.execute_reply": "2023-11-17T02:25:38.440283Z",
     "shell.execute_reply.started": "2023-11-17T02:23:23.327609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.1.1%2Bcu118-cp310-cp310-linux_x86_64.whl (2325.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m393.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.15.1)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.16.1%2Bcu118-cp310-cp310-linux_x86_64.whl (6.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.0.1)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.1%2Bcu118-cp310-cp310-linux_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Collecting triton==2.1.0 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: triton, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.0\n",
      "    Uninstalling torch-2.0.0:\n",
      "      Successfully uninstalled torch-2.0.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.15.1\n",
      "    Uninstalling torchvision-0.15.1:\n",
      "      Successfully uninstalled torchvision-0.15.1\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.0.1\n",
      "    Uninstalling torchaudio-2.0.1:\n",
      "      Successfully uninstalled torchaudio-2.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchdata 0.6.0 requires torch==2.0.0, but you have torch 2.1.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.1.1+cu118 torchaudio-2.1.1+cu118 torchvision-0.16.1+cu118 triton-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --upgrade --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T02:25:38.443136Z",
     "iopub.status.busy": "2023-11-17T02:25:38.442814Z",
     "iopub.status.idle": "2023-11-17T02:25:47.237818Z",
     "shell.execute_reply": "2023-11-17T02:25:47.236837Z",
     "shell.execute_reply.started": "2023-11-17T02:25:38.443093Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from lightning.fabric.loggers import CSVLogger\n",
    "from lightning.fabric.strategies import FSDPStrategy\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# # support running without installing as a package\n",
    "# wd = Path(__file__).parent.parent.resolve()\n",
    "# sys.path.append(str(wd))\n",
    "\n",
    "from tsai_gpt.model import GPT, Block, Config\n",
    "from tsai_gpt.packed_dataset import CombinedDataset, PackedDataset\n",
    "from tsai_gpt.speed_monitor import SpeedMonitorBase, estimate_flops, measure_flops\n",
    "from tsai_gpt.speed_monitor import SpeedMonitorFabric as SpeedMonitor\n",
    "from tsai_gpt.utils import chunked_cross_entropy, get_default_supported_precision, num_parameters, load_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T02:25:47.239421Z",
     "iopub.status.busy": "2023-11-17T02:25:47.238964Z",
     "iopub.status.idle": "2023-11-17T02:25:47.244272Z",
     "shell.execute_reply": "2023-11-17T02:25:47.243256Z",
     "shell.execute_reply.started": "2023-11-17T02:25:47.239393Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"pythia-160m\"\n",
    "name = \"redpajama\"\n",
    "out_dir = Path(\"out\") / name\n",
    "save_interval = 1000\n",
    "eval_interval = 1000\n",
    "eval_iters = 100\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T02:25:47.245784Z",
     "iopub.status.busy": "2023-11-17T02:25:47.245461Z",
     "iopub.status.idle": "2023-11-17T02:25:49.292055Z",
     "shell.execute_reply": "2023-11-17T02:25:49.290992Z",
     "shell.execute_reply.started": "2023-11-17T02:25:47.245758Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 6e-3\n",
    "batch_size = 32\n",
    "micro_batch_size = 4\n",
    "gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "assert gradient_accumulation_steps > 0\n",
    "#max_iters = 600000  # num_epochs * (epoch_size // micro_batch_size) // devices\n",
    "max_iters = 15000\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0\n",
    "decay_lr = True\n",
    "warmup_iters = 2000\n",
    "lr_decay_iters = max_iters\n",
    "min_lr = 6e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T02:25:49.293544Z",
     "iopub.status.busy": "2023-11-17T02:25:49.293183Z",
     "iopub.status.idle": "2023-11-17T02:25:49.302293Z",
     "shell.execute_reply": "2023-11-17T02:25:49.301342Z",
     "shell.execute_reply.started": "2023-11-17T02:25:49.293505Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data proportions from https://arxiv.org/pdf/2302.13971.pdf Table 1\n",
    "data_config = [\n",
    "    (\"arxiv\", 2.5),\n",
    "    (\"book\", 4.5),\n",
    "    (\"c4\", 15.0),\n",
    "    (\"cc\", 67.0),\n",
    "    (\"github\", 4.5),\n",
    "    (\"stackexchange\", 2.0),\n",
    "    (\"wikipedia\", 4.5),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T02:25:49.303734Z",
     "iopub.status.busy": "2023-11-17T02:25:49.303171Z",
     "iopub.status.idle": "2023-11-17T02:25:49.316555Z",
     "shell.execute_reply": "2023-11-17T02:25:49.315562Z",
     "shell.execute_reply.started": "2023-11-17T02:25:49.303700Z"
    }
   },
   "outputs": [],
   "source": [
    "hparams = {k: v for k, v in locals().items() if isinstance(v, (int, float, str)) and not k.startswith(\"_\")}\n",
    "logger = CSVLogger(\"out\", name, flush_logs_every_n_steps=log_interval)\n",
    "\n",
    "\n",
    "def setup(\n",
    "    devices: int = 4,\n",
    "    train_data_dir: Path = Path(\"data/redpajama_sample\"),\n",
    "    val_data_dir: Optional[Path] = None,\n",
    "    precision: Optional[str] = None,\n",
    "    resume: Union[bool, Path] = False,\n",
    ") -> None:\n",
    "    precision = precision or get_default_supported_precision(training=True)\n",
    "\n",
    "    if devices > 1:\n",
    "        strategy = FSDPStrategy(\n",
    "            auto_wrap_policy={Block},\n",
    "            activation_checkpointing_policy={Block},\n",
    "            state_dict_type=\"full\",\n",
    "            limit_all_gathers=True,\n",
    "            cpu_offload=False,\n",
    "        )\n",
    "    else:\n",
    "        strategy = \"auto\"\n",
    "\n",
    "    fabric = L.Fabric(devices=devices, strategy=strategy, precision=precision, loggers=logger)\n",
    "    fabric.print(hparams)\n",
    "    fabric.launch(main, train_data_dir, val_data_dir, resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T02:25:49.318008Z",
     "iopub.status.busy": "2023-11-17T02:25:49.317685Z",
     "iopub.status.idle": "2023-11-17T02:25:49.331490Z",
     "shell.execute_reply": "2023-11-17T02:25:49.330560Z",
     "shell.execute_reply.started": "2023-11-17T02:25:49.317981Z"
    }
   },
   "outputs": [],
   "source": [
    "model_copy = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T02:25:49.333659Z",
     "iopub.status.busy": "2023-11-17T02:25:49.333150Z",
     "iopub.status.idle": "2023-11-17T02:25:49.352759Z",
     "shell.execute_reply": "2023-11-17T02:25:49.351653Z",
     "shell.execute_reply.started": "2023-11-17T02:25:49.333574Z"
    }
   },
   "outputs": [],
   "source": [
    "def main(fabric: L.Fabric, train_data_dir: Path, val_data_dir: Path, resume: Union[bool, Path]) -> None:\n",
    "    global model_copy\n",
    "    speed_monitor = SpeedMonitor(fabric, window_size=50, time_unit=\"seconds\")\n",
    "\n",
    "    if fabric.global_rank == 0:\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    config = Config.from_name(model_name)\n",
    "\n",
    "    train_dataloader, val_dataloader = create_dataloaders(\n",
    "        batch_size=micro_batch_size,\n",
    "        block_size=config.block_size,\n",
    "        fabric=fabric,\n",
    "        train_data_dir=train_data_dir,\n",
    "        val_data_dir=val_data_dir,\n",
    "        seed=(1337 + fabric.global_rank),\n",
    "    )\n",
    "    if val_dataloader is None:\n",
    "        train_dataloader = fabric.setup_dataloaders(train_dataloader)\n",
    "    else:\n",
    "        train_dataloader, val_dataloader = fabric.setup_dataloaders(train_dataloader, val_dataloader)\n",
    "\n",
    "    fabric.seed_everything(1337)  # same seed for every process to init model (FSDP)\n",
    "\n",
    "    fabric.print(f\"Loading model with {config.__dict__}\")\n",
    "    t0 = time.perf_counter()\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    def _init_weights(module: nn.Module) -> None:\n",
    "            \"\"\"Meant to be used with `gpt.apply(gpt._init_weights)`.\"\"\"\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    with fabric.init_module(empty_init=True):\n",
    "        model = GPT(config)\n",
    "        model.apply(_init_weights)\n",
    "    model.apply(_init_weights)\n",
    "\n",
    "    \n",
    "    # checkpoint_path = Path(\"out/redpajama/iter-000999-ckpt.pth\")\n",
    "\n",
    "    # load_checkpoint(fabric, model, checkpoint_path)\n",
    "        \n",
    "    # print(model.transformer.h[0].mlp.fc.weight)\n",
    "\n",
    "    fabric.print(f\"Time to instantiate model: {time.perf_counter() - t0:.02f} seconds.\")\n",
    "    fabric.print(f\"Total parameters {num_parameters(model):,}\")\n",
    "\n",
    "    model = fabric.setup(model)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(beta1, beta2), foreach=False\n",
    "    )\n",
    "\n",
    "    # model_copy = model\n",
    "\n",
    "    optimizer = fabric.setup_optimizers(optimizer)\n",
    "\n",
    "    state = {\"model\": model, \"optimizer\": optimizer, \"hparams\": hparams, \"iter_num\": 0, \"step_count\": 0}\n",
    "\n",
    "    if resume is True:\n",
    "        resume = max(out_dir.glob(\"*.pth\"), key=lambda p: int(p.name.split(\"-\")[1]))\n",
    "    if resume:\n",
    "        fabric.print(f\"Resuming training from {resume}\")\n",
    "        fabric.load(resume, state)\n",
    "\n",
    "    train_time = time.perf_counter()\n",
    "    train(fabric, state, train_dataloader, val_dataloader, speed_monitor)\n",
    "    fabric.print(f\"Training time: {(time.perf_counter()-train_time):.2f}s\")\n",
    "    if fabric.device.type == \"cuda\":\n",
    "        fabric.print(f\"Memory used: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T02:25:49.355202Z",
     "iopub.status.busy": "2023-11-17T02:25:49.354410Z",
     "iopub.status.idle": "2023-11-17T02:25:49.374977Z",
     "shell.execute_reply": "2023-11-17T02:25:49.373849Z",
     "shell.execute_reply.started": "2023-11-17T02:25:49.355161Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    fabric: L.Fabric,\n",
    "    state: dict,\n",
    "    train_dataloader: DataLoader,\n",
    "    val_dataloader: DataLoader,\n",
    "    speed_monitor: SpeedMonitorBase,\n",
    ") -> None:\n",
    "    model = state[\"model\"]\n",
    "    optimizer = state[\"optimizer\"]\n",
    "\n",
    "    if val_dataloader is not None:\n",
    "        validate(fabric, model, val_dataloader)  # sanity check\n",
    "\n",
    "    with torch.device(\"meta\"):\n",
    "        meta_model = GPT(model.config)\n",
    "        # \"estimated\" is not as precise as \"measured\". Estimated is optimistic but widely used in the wild.\n",
    "        # When comparing MFU or FLOP numbers with other projects that use estimated FLOPs,\n",
    "        # consider passing `SpeedMonitor(flops_per_batch=estimated_flops)` instead\n",
    "        estimated_flops = estimate_flops(meta_model) * micro_batch_size\n",
    "        fabric.print(f\"Estimated TFLOPs: {estimated_flops * fabric.world_size / 1e12:.2f}\")\n",
    "        x = torch.randint(0, 1, (micro_batch_size, model.max_seq_length))\n",
    "        measured_flops = measure_flops(meta_model, x)\n",
    "        fabric.print(f\"Measured TFLOPs: {measured_flops * fabric.world_size / 1e12:.2f}\")\n",
    "        del meta_model, x\n",
    "\n",
    "    total_lengths = 0\n",
    "    total_t0 = time.perf_counter()\n",
    "\n",
    "    for state[\"iter_num\"], train_data in enumerate(train_dataloader, state[\"iter_num\"]):\n",
    "        if state[\"iter_num\"] >= max_iters:\n",
    "            checkpoint_path = out_dir / f\"iter-{state['iter_num']:06d}-ckpt.pth\"\n",
    "            fabric.print(f\"Saving checkpoint to {str(checkpoint_path)!r}\")\n",
    "            fabric.save(checkpoint_path, state)\n",
    "            break\n",
    "\n",
    "        # determine and set the learning rate for this iteration\n",
    "        lr = get_lr(state[\"iter_num\"]) if decay_lr else learning_rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "        iter_t0 = time.perf_counter()\n",
    "\n",
    "        input_ids = train_data[:, 0 : model.max_seq_length].contiguous()\n",
    "        targets = train_data[:, 1 : model.max_seq_length + 1].contiguous()\n",
    "\n",
    "        is_accumulating = (state[\"iter_num\"] + 1) % gradient_accumulation_steps != 0\n",
    "        with fabric.no_backward_sync(model, enabled=is_accumulating):\n",
    "            logits = model(input_ids)\n",
    "            loss = chunked_cross_entropy(logits, targets, chunk_size=0)\n",
    "            fabric.backward(loss / gradient_accumulation_steps)\n",
    "        \n",
    "        # return \n",
    "\n",
    "        if not is_accumulating:\n",
    "            fabric.clip_gradients(model, optimizer, max_norm=grad_clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            state[\"step_count\"] += 1\n",
    "\n",
    "        t1 = time.perf_counter()\n",
    "        total_lengths += input_ids.size(1)\n",
    "        speed_monitor.on_train_batch_end(\n",
    "            (state[\"iter_num\"] + 1) * micro_batch_size,\n",
    "            t1 - total_t0,\n",
    "            # this assumes that device FLOPs are the same and that all devices have the same batch size\n",
    "            fabric.world_size,\n",
    "            flops_per_batch=measured_flops,\n",
    "            lengths=total_lengths,\n",
    "        )\n",
    "        if state[\"iter_num\"] % log_interval == 0:\n",
    "            fabric.print(\n",
    "                f\"iter {state['iter_num']} step {state['step_count']}: loss {loss.item():.4f}, LR: {lr:.6f}, iter time:\"\n",
    "                f\" {(t1 - iter_t0) * 1000:.2f}ms{' (optimizer.step)' if not is_accumulating else ''}\"\n",
    "            )\n",
    "\n",
    "        if val_dataloader is not None and not is_accumulating and state[\"step_count\"] % eval_interval == 0:\n",
    "            t0 = time.perf_counter()\n",
    "            val_loss = validate(fabric, model, val_dataloader)\n",
    "            t1 = time.perf_counter() - t0\n",
    "            speed_monitor.eval_end(t1)\n",
    "            fabric.print(f\"step {state['iter_num']}: val loss {val_loss.item():.4f}, val time: {t1 * 1000:.2f}ms\")\n",
    "            fabric.barrier()\n",
    "        if not is_accumulating and state[\"step_count\"] % save_interval == 0:\n",
    "            checkpoint_path = out_dir / f\"iter-{state['iter_num']:06d}-ckpt.pth\"\n",
    "            fabric.print(f\"Saving checkpoint to {str(checkpoint_path)!r}\")\n",
    "            fabric.save(checkpoint_path, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T02:25:49.377144Z",
     "iopub.status.busy": "2023-11-17T02:25:49.376437Z",
     "iopub.status.idle": "2023-11-17T02:25:49.389183Z",
     "shell.execute_reply": "2023-11-17T02:25:49.388203Z",
     "shell.execute_reply.started": "2023-11-17T02:25:49.377107Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def validate(fabric: L.Fabric, model: torch.nn.Module, val_dataloader: DataLoader) -> torch.Tensor:\n",
    "    fabric.print(\"Validating ...\")\n",
    "    model.eval()\n",
    "\n",
    "    losses = torch.zeros(eval_iters, device=fabric.device)\n",
    "    for k, val_data in enumerate(val_dataloader):\n",
    "        input_ids = val_data[:, 0 : model.max_seq_length].contiguous()\n",
    "        targets = val_data[:, 1 : model.max_seq_length + 1].contiguous()\n",
    "        logits = model(input_ids)\n",
    "        losses[k] = chunked_cross_entropy(logits, targets, chunk_size=0)\n",
    "    out = losses.mean()\n",
    "\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T02:25:49.390933Z",
     "iopub.status.busy": "2023-11-17T02:25:49.390553Z",
     "iopub.status.idle": "2023-11-17T02:25:49.405118Z",
     "shell.execute_reply": "2023-11-17T02:25:49.404135Z",
     "shell.execute_reply.started": "2023-11-17T02:25:49.390904Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataloader(\n",
    "    batch_size: int, block_size: int, data_dir: Path, fabric: L.Fabric, shuffle: bool = True, seed: int = 12345\n",
    ") -> DataLoader:\n",
    "    datasets = []\n",
    "    for prefix, _ in data_config:\n",
    "        filenames = glob.glob(str(data_dir / f\"{prefix}*\"))\n",
    "        dataset = PackedDataset(\n",
    "            filenames,\n",
    "            n_chunks=4,\n",
    "            block_size=block_size,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            num_processes=fabric.world_size,\n",
    "            process_rank=fabric.global_rank,\n",
    "        )\n",
    "        datasets.append(dataset)\n",
    "\n",
    "    if not datasets:\n",
    "        raise RuntimeError(\n",
    "            f\"No data found at {data_dir}. Make sure you ran prepare_redpajama.py to create the dataset.\"\n",
    "        )\n",
    "\n",
    "    weights = [weight for _, weight in data_config]\n",
    "    sum_weights = sum(weights)\n",
    "    weights = [el / sum_weights for el in weights]\n",
    "\n",
    "    combined_dataset = CombinedDataset(datasets=datasets, seed=seed, weights=weights)\n",
    "\n",
    "    return DataLoader(combined_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T02:25:49.406797Z",
     "iopub.status.busy": "2023-11-17T02:25:49.406339Z",
     "iopub.status.idle": "2023-11-17T02:25:49.417045Z",
     "shell.execute_reply": "2023-11-17T02:25:49.416089Z",
     "shell.execute_reply.started": "2023-11-17T02:25:49.406757Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataloaders(\n",
    "    batch_size: int,\n",
    "    block_size: int,\n",
    "    fabric: L.Fabric,\n",
    "    train_data_dir: Path = Path(\"data/redpajama_sample\"),\n",
    "    val_data_dir: Optional[Path] = None,\n",
    "    seed: int = 12345,\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    # Increase by one because we need the next word as well\n",
    "    effective_block_size = block_size + 1\n",
    "    train_dataloader = create_dataloader(\n",
    "        batch_size=batch_size,\n",
    "        block_size=effective_block_size,\n",
    "        fabric=fabric,\n",
    "        data_dir=train_data_dir,\n",
    "        shuffle=True,\n",
    "        seed=seed,\n",
    "    )\n",
    "    val_dataloader = (\n",
    "        create_dataloader(\n",
    "            batch_size=batch_size,\n",
    "            block_size=effective_block_size,\n",
    "            fabric=fabric,\n",
    "            data_dir=val_data_dir,\n",
    "            shuffle=False,\n",
    "            seed=seed,\n",
    "        )\n",
    "        if val_data_dir\n",
    "        else None\n",
    "    )\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T02:25:49.418509Z",
     "iopub.status.busy": "2023-11-17T02:25:49.418153Z",
     "iopub.status.idle": "2023-11-17T02:25:49.432024Z",
     "shell.execute_reply": "2023-11-17T02:25:49.431180Z",
     "shell.execute_reply.started": "2023-11-17T02:25:49.418462Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_lr(it: int) -> float:\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T02:25:49.433415Z",
     "iopub.status.busy": "2023-11-17T02:25:49.433122Z",
     "iopub.status.idle": "2023-11-17T04:50:34.532031Z",
     "shell.execute_reply": "2023-11-17T04:50:34.530876Z",
     "shell.execute_reply.started": "2023-11-17T02:25:49.433389Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using 16-bit Automatic Mixed Precision (AMP)\n",
      "INFO: Seed set to 1337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_link': 'https://drive.google.com/file/d/1z9kioO0GtxBZV-BqUEKT-2NT_mjw3uFj/view?usp=drive_link', 'output_file_path': '/kaggle/working/downloads/', 'folder_path': '/kaggle/working/downloads', 'model_name': 'pythia-160m', 'name': 'redpajama', 'save_interval': 1000, 'eval_interval': 1000, 'eval_iters': 100, 'log_interval': 100, 'learning_rate': 0.006, 'batch_size': 32, 'micro_batch_size': 4, 'gradient_accumulation_steps': 8, 'max_iters': 15000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 2000, 'lr_decay_iters': 15000, 'min_lr': 6e-06}\n",
      "Loading model with {'name': 'pythia-160m', 'hf_config': {'org': 'EleutherAI', 'name': 'pythia-160m-deduped'}, 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 128, 'padded_vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'lm_head_bias': False, 'n_query_groups': 12, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'none', 'intermediate_size': 3072, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 64, 'rope_n_elem': 16}\n",
      "Time to instantiate model: 2.48 seconds.\n",
      "Total parameters 162,322,944\n",
      "Estimated TFLOPs: 11.07\n",
      "Measured TFLOPs: 7.93\n",
      "iter 0 step 0: loss 10.9643, LR: 0.000000, iter time: 3325.12ms\n",
      "iter 100 step 12: loss 8.3226, LR: 0.000300, iter time: 530.16ms\n",
      "iter 200 step 25: loss 7.0596, LR: 0.000600, iter time: 588.00ms\n",
      "iter 300 step 37: loss 6.5436, LR: 0.000900, iter time: 606.09ms\n",
      "iter 400 step 50: loss 6.7563, LR: 0.001200, iter time: 603.25ms\n",
      "iter 500 step 62: loss 6.0913, LR: 0.001500, iter time: 608.93ms\n",
      "iter 600 step 75: loss 6.0968, LR: 0.001800, iter time: 608.76ms\n",
      "iter 700 step 87: loss 6.5729, LR: 0.002100, iter time: 606.58ms\n",
      "iter 800 step 100: loss 5.7721, LR: 0.002400, iter time: 602.84ms\n",
      "iter 900 step 112: loss 6.0011, LR: 0.002700, iter time: 595.16ms\n",
      "iter 1000 step 125: loss 5.9299, LR: 0.003000, iter time: 608.38ms\n",
      "iter 1100 step 137: loss 5.7553, LR: 0.003300, iter time: 598.65ms\n",
      "iter 1200 step 150: loss 5.7651, LR: 0.003600, iter time: 602.71ms\n",
      "iter 1300 step 162: loss 5.5207, LR: 0.003900, iter time: 590.34ms\n",
      "iter 1400 step 175: loss 5.6293, LR: 0.004200, iter time: 595.72ms\n",
      "iter 1500 step 187: loss 5.4690, LR: 0.004500, iter time: 587.37ms\n",
      "iter 1600 step 200: loss 6.4060, LR: 0.004800, iter time: 600.85ms\n",
      "iter 1700 step 212: loss 5.5225, LR: 0.005100, iter time: 583.30ms\n",
      "iter 1800 step 225: loss 6.0416, LR: 0.005400, iter time: 599.52ms\n",
      "iter 1900 step 237: loss 5.4506, LR: 0.005700, iter time: 574.02ms\n",
      "iter 2000 step 250: loss 5.5431, LR: 0.006000, iter time: 592.00ms\n",
      "iter 2100 step 262: loss 5.4627, LR: 0.005999, iter time: 572.57ms\n",
      "iter 2200 step 275: loss 4.9885, LR: 0.005997, iter time: 582.70ms\n",
      "iter 2300 step 287: loss 5.5306, LR: 0.005992, iter time: 572.49ms\n",
      "iter 2400 step 300: loss 5.3439, LR: 0.005986, iter time: 582.34ms\n",
      "iter 2500 step 312: loss 5.1694, LR: 0.005978, iter time: 555.92ms\n",
      "iter 2600 step 325: loss 5.4220, LR: 0.005969, iter time: 577.43ms\n",
      "iter 2700 step 337: loss 5.0807, LR: 0.005957, iter time: 568.07ms\n",
      "iter 2800 step 350: loss 5.0365, LR: 0.005944, iter time: 585.10ms\n",
      "iter 2900 step 362: loss 5.1962, LR: 0.005929, iter time: 570.38ms\n",
      "iter 3000 step 375: loss 4.8579, LR: 0.005913, iter time: 577.24ms\n",
      "iter 3100 step 387: loss 5.1866, LR: 0.005895, iter time: 566.37ms\n",
      "iter 3200 step 400: loss 5.6523, LR: 0.005875, iter time: 581.26ms\n",
      "iter 3300 step 412: loss 5.5315, LR: 0.005853, iter time: 558.54ms\n",
      "iter 3400 step 425: loss 5.4156, LR: 0.005830, iter time: 579.38ms\n",
      "iter 3500 step 437: loss 5.3368, LR: 0.005805, iter time: 561.92ms\n",
      "iter 3600 step 450: loss 5.0795, LR: 0.005779, iter time: 579.46ms\n",
      "iter 3700 step 462: loss 5.2605, LR: 0.005751, iter time: 560.58ms\n",
      "iter 3800 step 475: loss 4.9780, LR: 0.005721, iter time: 577.67ms\n",
      "iter 3900 step 487: loss 4.9794, LR: 0.005690, iter time: 562.90ms\n",
      "iter 4000 step 500: loss 5.1516, LR: 0.005657, iter time: 574.04ms\n",
      "iter 4100 step 512: loss 5.0548, LR: 0.005622, iter time: 561.41ms\n",
      "iter 4200 step 525: loss 5.0116, LR: 0.005586, iter time: 576.92ms\n",
      "iter 4300 step 537: loss 4.7468, LR: 0.005549, iter time: 557.79ms\n",
      "iter 4400 step 550: loss 5.2710, LR: 0.005510, iter time: 578.04ms\n",
      "iter 4500 step 562: loss 4.8342, LR: 0.005469, iter time: 553.98ms\n",
      "iter 4600 step 575: loss 5.0762, LR: 0.005428, iter time: 573.65ms\n",
      "iter 4700 step 587: loss 4.9422, LR: 0.005384, iter time: 560.89ms\n",
      "iter 4800 step 600: loss 4.7763, LR: 0.005340, iter time: 574.26ms\n",
      "iter 4900 step 612: loss 4.7848, LR: 0.005294, iter time: 561.21ms\n",
      "iter 5000 step 625: loss 4.9698, LR: 0.005246, iter time: 575.26ms\n",
      "iter 5100 step 637: loss 5.1561, LR: 0.005198, iter time: 560.30ms\n",
      "iter 5200 step 650: loss 4.8690, LR: 0.005148, iter time: 574.52ms\n",
      "iter 5300 step 662: loss 4.8476, LR: 0.005096, iter time: 554.20ms\n",
      "iter 5400 step 675: loss 4.5908, LR: 0.005044, iter time: 567.23ms\n",
      "iter 5500 step 687: loss 5.0293, LR: 0.004990, iter time: 562.98ms\n",
      "iter 5600 step 700: loss 4.8811, LR: 0.004936, iter time: 575.33ms\n",
      "iter 5700 step 712: loss 4.9950, LR: 0.004880, iter time: 560.29ms\n",
      "iter 5800 step 725: loss 4.8720, LR: 0.004823, iter time: 575.63ms\n",
      "iter 5900 step 737: loss 4.8730, LR: 0.004765, iter time: 554.39ms\n",
      "iter 6000 step 750: loss 5.2064, LR: 0.004705, iter time: 573.61ms\n",
      "iter 6100 step 762: loss 4.8222, LR: 0.004645, iter time: 559.17ms\n",
      "iter 6200 step 775: loss 4.7224, LR: 0.004584, iter time: 573.57ms\n",
      "iter 6300 step 787: loss 4.3220, LR: 0.004522, iter time: 561.61ms\n",
      "iter 6400 step 800: loss 4.8054, LR: 0.004459, iter time: 575.91ms\n",
      "iter 6500 step 812: loss 4.9915, LR: 0.004396, iter time: 560.97ms\n",
      "iter 6600 step 825: loss 4.9840, LR: 0.004331, iter time: 578.39ms\n",
      "iter 6700 step 837: loss 4.7345, LR: 0.004266, iter time: 563.44ms\n",
      "iter 6800 step 850: loss 4.9924, LR: 0.004200, iter time: 572.23ms\n",
      "iter 6900 step 862: loss 4.3236, LR: 0.004133, iter time: 563.10ms\n",
      "iter 7000 step 875: loss 4.8405, LR: 0.004066, iter time: 575.02ms\n",
      "iter 7100 step 887: loss 4.7023, LR: 0.003998, iter time: 554.63ms\n",
      "iter 7200 step 900: loss 4.9665, LR: 0.003929, iter time: 575.62ms\n",
      "iter 7300 step 912: loss 4.7832, LR: 0.003860, iter time: 561.55ms\n",
      "iter 7400 step 925: loss 4.7465, LR: 0.003790, iter time: 573.69ms\n",
      "iter 7500 step 937: loss 4.7097, LR: 0.003720, iter time: 549.51ms\n",
      "iter 7600 step 950: loss 4.9579, LR: 0.003650, iter time: 579.28ms\n",
      "iter 7700 step 962: loss 4.7773, LR: 0.003579, iter time: 560.23ms\n",
      "iter 7800 step 975: loss 4.4469, LR: 0.003508, iter time: 575.85ms\n",
      "iter 7900 step 987: loss 4.7394, LR: 0.003436, iter time: 558.72ms\n",
      "Saving checkpoint to 'out/redpajama/iter-007999-ckpt.pth'\n",
      "iter 8000 step 1000: loss 4.6743, LR: 0.003364, iter time: 598.36ms\n",
      "iter 8100 step 1012: loss 4.6426, LR: 0.003292, iter time: 559.10ms\n",
      "iter 8200 step 1025: loss 4.7722, LR: 0.003220, iter time: 575.13ms\n",
      "iter 8300 step 1037: loss 4.9071, LR: 0.003148, iter time: 558.95ms\n",
      "iter 8400 step 1050: loss 4.7031, LR: 0.003075, iter time: 577.68ms\n",
      "iter 8500 step 1062: loss 4.7218, LR: 0.003003, iter time: 549.76ms\n",
      "iter 8600 step 1075: loss 4.8605, LR: 0.002931, iter time: 575.90ms\n",
      "iter 8700 step 1087: loss 4.6768, LR: 0.002858, iter time: 561.04ms\n",
      "iter 8800 step 1100: loss 4.9993, LR: 0.002786, iter time: 575.64ms\n",
      "iter 8900 step 1112: loss 4.5923, LR: 0.002714, iter time: 558.06ms\n",
      "iter 9000 step 1125: loss 4.3013, LR: 0.002642, iter time: 576.75ms\n",
      "iter 9100 step 1137: loss 4.6935, LR: 0.002570, iter time: 562.81ms\n",
      "iter 9200 step 1150: loss 4.5061, LR: 0.002498, iter time: 570.53ms\n",
      "iter 9300 step 1162: loss 3.7384, LR: 0.002427, iter time: 557.70ms\n",
      "iter 9400 step 1175: loss 4.7853, LR: 0.002356, iter time: 572.29ms\n",
      "iter 9500 step 1187: loss 4.6083, LR: 0.002286, iter time: 560.96ms\n",
      "iter 9600 step 1200: loss 4.2127, LR: 0.002216, iter time: 573.66ms\n",
      "iter 9700 step 1212: loss 4.2827, LR: 0.002146, iter time: 558.17ms\n",
      "iter 9800 step 1225: loss 4.7150, LR: 0.002077, iter time: 580.71ms\n",
      "iter 9900 step 1237: loss 4.0608, LR: 0.002008, iter time: 553.07ms\n",
      "iter 10000 step 1250: loss 4.5615, LR: 0.001940, iter time: 575.99ms\n",
      "iter 10100 step 1262: loss 4.6350, LR: 0.001873, iter time: 561.29ms\n",
      "iter 10200 step 1275: loss 4.6565, LR: 0.001806, iter time: 578.00ms\n",
      "iter 10300 step 1287: loss 4.4539, LR: 0.001740, iter time: 561.45ms\n",
      "iter 10400 step 1300: loss 4.5116, LR: 0.001675, iter time: 576.43ms\n",
      "iter 10500 step 1312: loss 4.7322, LR: 0.001610, iter time: 562.21ms\n",
      "iter 10600 step 1325: loss 4.4263, LR: 0.001547, iter time: 576.89ms\n",
      "iter 10700 step 1337: loss 4.8405, LR: 0.001484, iter time: 560.90ms\n",
      "iter 10800 step 1350: loss 4.4276, LR: 0.001422, iter time: 578.20ms\n",
      "iter 10900 step 1362: loss 4.4072, LR: 0.001361, iter time: 563.87ms\n",
      "iter 11000 step 1375: loss 4.4017, LR: 0.001301, iter time: 578.54ms\n",
      "iter 11100 step 1387: loss 4.3127, LR: 0.001241, iter time: 560.60ms\n",
      "iter 11200 step 1400: loss 4.3161, LR: 0.001183, iter time: 580.34ms\n",
      "iter 11300 step 1412: loss 4.2941, LR: 0.001126, iter time: 562.55ms\n",
      "iter 11400 step 1425: loss 4.4106, LR: 0.001070, iter time: 576.52ms\n",
      "iter 11500 step 1437: loss 4.3453, LR: 0.001016, iter time: 560.12ms\n",
      "iter 11600 step 1450: loss 4.0662, LR: 0.000962, iter time: 579.15ms\n",
      "iter 11700 step 1462: loss 4.0552, LR: 0.000910, iter time: 561.63ms\n",
      "iter 11800 step 1475: loss 4.5213, LR: 0.000858, iter time: 576.76ms\n",
      "iter 11900 step 1487: loss 4.4588, LR: 0.000808, iter time: 565.00ms\n",
      "iter 12000 step 1500: loss 4.2618, LR: 0.000760, iter time: 580.30ms\n",
      "iter 12100 step 1512: loss 4.2741, LR: 0.000712, iter time: 565.38ms\n",
      "iter 12200 step 1525: loss 4.1834, LR: 0.000666, iter time: 578.67ms\n",
      "iter 12300 step 1537: loss 4.1628, LR: 0.000622, iter time: 561.05ms\n",
      "iter 12400 step 1550: loss 4.3587, LR: 0.000578, iter time: 578.12ms\n",
      "iter 12500 step 1562: loss 4.0581, LR: 0.000537, iter time: 562.92ms\n",
      "iter 12600 step 1575: loss 4.3622, LR: 0.000496, iter time: 582.89ms\n",
      "iter 12700 step 1587: loss 4.1392, LR: 0.000457, iter time: 562.35ms\n",
      "iter 12800 step 1600: loss 4.2754, LR: 0.000420, iter time: 577.92ms\n",
      "iter 12900 step 1612: loss 4.5487, LR: 0.000384, iter time: 563.10ms\n",
      "iter 13000 step 1625: loss 4.5171, LR: 0.000349, iter time: 572.72ms\n",
      "iter 13100 step 1637: loss 4.3754, LR: 0.000316, iter time: 564.34ms\n",
      "iter 13200 step 1650: loss 4.4985, LR: 0.000285, iter time: 580.78ms\n",
      "iter 13300 step 1662: loss 3.2125, LR: 0.000255, iter time: 559.78ms\n",
      "iter 13400 step 1675: loss 4.1128, LR: 0.000227, iter time: 578.87ms\n",
      "iter 13500 step 1687: loss 4.6506, LR: 0.000201, iter time: 558.86ms\n",
      "iter 13600 step 1700: loss 4.2759, LR: 0.000176, iter time: 579.71ms\n",
      "iter 13700 step 1712: loss 3.5882, LR: 0.000153, iter time: 564.09ms\n",
      "iter 13800 step 1725: loss 3.5937, LR: 0.000131, iter time: 576.06ms\n",
      "iter 13900 step 1737: loss 4.1158, LR: 0.000111, iter time: 562.78ms\n",
      "iter 14000 step 1750: loss 4.1648, LR: 0.000093, iter time: 574.20ms\n",
      "iter 14100 step 1762: loss 4.1708, LR: 0.000077, iter time: 561.64ms\n",
      "iter 14200 step 1775: loss 4.2506, LR: 0.000062, iter time: 580.79ms\n",
      "iter 14300 step 1787: loss 4.1807, LR: 0.000049, iter time: 564.41ms\n",
      "iter 14400 step 1800: loss 4.2685, LR: 0.000037, iter time: 575.39ms\n",
      "iter 14500 step 1812: loss 4.4177, LR: 0.000028, iter time: 560.03ms\n",
      "iter 14600 step 1825: loss 3.6783, LR: 0.000020, iter time: 578.68ms\n",
      "iter 14700 step 1837: loss 4.2052, LR: 0.000014, iter time: 565.27ms\n",
      "iter 14800 step 1850: loss 4.3321, LR: 0.000009, iter time: 578.23ms\n",
      "iter 14900 step 1862: loss 3.7738, LR: 0.000007, iter time: 558.23ms\n",
      "Saving checkpoint to 'out/redpajama/iter-015000-ckpt.pth'\n",
      "Training time: 8682.24s\n",
      "Memory used: 12.25 GB\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "setup(\n",
    "    devices=1,\n",
    "    train_data_dir=Path(\"data/lit-redpajama-sample\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T04:51:25.930354Z",
     "iopub.status.busy": "2023-11-17T04:51:25.929912Z",
     "iopub.status.idle": "2023-11-17T04:51:25.944020Z",
     "shell.execute_reply": "2023-11-17T04:51:25.942935Z",
     "shell.execute_reply.started": "2023-11-17T04:51:25.930321Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate(\n",
    "    model: GPT,\n",
    "    idx: torch.Tensor,\n",
    "    max_returned_tokens: int,\n",
    "    *,\n",
    "    temperature: float = 1.0,\n",
    "    top_k:int = None,\n",
    "    eos_id:int = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as requested.\n",
    "    The implementation of this function is modified from A. Karpathy's nanoGPT.\n",
    "    Args:\n",
    "        model: The model to use.\n",
    "        idx: Tensor of shape (T) with indices of the prompt sequence.\n",
    "        max_returned_tokens: The maximum number of tokens to return (given plus generated).\n",
    "        temperature: Scales the predicted logits by 1 / temperature.\n",
    "        top_k: If specified, only sample among the tokens with the k highest probabilities.\n",
    "        eos_id: If specified, stop generating any more token once the <eos> token is triggered.\n",
    "    \"\"\"\n",
    "    T = idx.size(0)\n",
    "    assert max_returned_tokens > T\n",
    "    if model.max_seq_length < max_returned_tokens - 1:\n",
    "        # rolling the kv cache based on the `input_pos` value would be necessary. However, doing so would introduce a\n",
    "        # data dependency on the `input_pos` tensor and impact model compilation. Since this setting is uncommon, we do\n",
    "        # not support it to avoid negatively impacting the overall speed\n",
    "        raise NotImplementedError(f\"max_seq_length {model.max_seq_length} needs to be >= {max_returned_tokens - 1}\")\n",
    "\n",
    "    device, dtype = idx.device, idx.dtype\n",
    "    # create an empty tensor of the expected final shape and fill in the current tokens\n",
    "    empty = torch.empty(max_returned_tokens, dtype=dtype, device=device)\n",
    "    empty[:T] = idx\n",
    "    idx = empty\n",
    "    input_pos = torch.arange(0, T, device=device)\n",
    "\n",
    "    # generate up to a fixed number of tokens\n",
    "    for _ in range(max_returned_tokens - T):\n",
    "        x = idx.index_select(0, input_pos).view(1, -1)\n",
    "\n",
    "        # forward\n",
    "        logits = model(x, input_pos)\n",
    "        logits = logits[0, -1] / temperature\n",
    "\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits = torch.where(logits < v[[-1]], -float(\"Inf\"), logits)\n",
    "\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1).to(dtype=dtype)\n",
    "\n",
    "        # advance\n",
    "        input_pos = input_pos[-1:] + 1\n",
    "\n",
    "        # concatenate the new generation\n",
    "        idx = idx.index_copy(0, input_pos, idx_next)\n",
    "\n",
    "        # if <eos> token is triggered, return the output (stop generation)\n",
    "        if idx_next == eos_id:\n",
    "            return idx[:input_pos]  # include the EOS token\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T04:55:52.159742Z",
     "iopub.status.busy": "2023-11-17T04:55:52.158963Z",
     "iopub.status.idle": "2023-11-17T04:55:52.756016Z",
     "shell.execute_reply": "2023-11-17T04:55:52.754951Z",
     "shell.execute_reply.started": "2023-11-17T04:55:52.159707Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import lightning as L\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.fabric.loggers import CSVLogger\n",
    "from lightning.fabric.strategies import FSDPStrategy\n",
    "\n",
    "from tsai_gpt.model import GPT, Block, Config\n",
    "from tsai_gpt.tokenizer import Tokenizer\n",
    "from tsai_gpt.utils import get_default_supported_precision, load_checkpoint, gptq_quantization\n",
    "\n",
    "model_name = \"pythia-160m\"\n",
    "name = \"redpajama\"\n",
    "\n",
    "checkpoint_dir = Path(\"out/redpajama/iter-015000-ckpt.pth\")\n",
    "quantize = None\n",
    "strategy = \"auto\"\n",
    "devices = 1\n",
    "precision = get_default_supported_precision(training=False)\n",
    "plugins = None\n",
    "fabric = L.Fabric(devices=devices, precision=precision, strategy=strategy, plugins=plugins)\n",
    "fabric.launch()\n",
    "\n",
    "\n",
    "with fabric.init_module(empty_init=True), gptq_quantization(quantize==\"gptq.int4\"):\n",
    "    config = Config.from_name(model_name)\n",
    "    model = GPT(config)\n",
    "\n",
    "model.eval()\n",
    "model = fabric.setup_module(model)\n",
    "load_checkpoint(fabric, model, checkpoint_dir)\n",
    "\n",
    "tokenizer = Tokenizer(Path(''))\n",
    "\n",
    "\n",
    "def generate_dialogue(input_text, temperature, max_tokens, top_k):\n",
    "    encoded = tokenizer.encode(input_text, device=fabric.device)\n",
    "    max_returned_tokens = encoded.size(0) + max_tokens\n",
    "\n",
    "\n",
    "    with fabric.init_tensor():\n",
    "        # set the max_seq_length to limit the memory usage to what we need\n",
    "        model.max_seq_length = max_returned_tokens\n",
    "\n",
    "\n",
    "    with fabric.init_tensor():\n",
    "        model.set_kv_cache(batch_size=1)\n",
    "\n",
    "    y = generate(model, encoded, max_returned_tokens, temperature=temperature, top_k=top_k)\n",
    "\n",
    "    return(tokenizer.decode(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T04:57:50.712978Z",
     "iopub.status.busy": "2023-11-17T04:57:50.712584Z",
     "iopub.status.idle": "2023-11-17T04:57:55.972516Z",
     "shell.execute_reply": "2023-11-17T04:57:55.971517Z",
     "shell.execute_reply.started": "2023-11-17T04:57:50.712944Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quantization is key to advancing for managing and democratic power.\\nCourt documents the future, the research and methodology has been updated to 1960/155/15, and 2014 reported that the Eurasian system works together with the decision of working for the DIDI, the research group to design for the doctor, which he was the subject.\\nThe investigation, the research and accuracy of diagnostic laboratory procedures; the 211-shifting operations were incorporated from the evidence that was to be known as an aggression and the potential cases were tested and remained in cases to the overall high cost among the work.\\nThe study of the Eurasian system were classified in the 2012-2011. All the patients who were enrolled.\\nThe report of the LGBTQI, the 2014 issue of the CDETQI, the study was performed in the Atomic/Economic field, (i) was 1.4. It was also 1.4. These results included the duration of 1960/15. 58;00% from 2.22;0\\n$1/10/2015 Folden Lydricks\\nLots Bats b.b.b.b.b.b.b.b.b.p.b.b.b.aa.b.b.b.b.b.b.b.b.l.b.b.p.b.m.b.d.b.b.b.d.b.g.b.l.d.b.d.b.b.p.d.b.b.d.d.d.b.b.d.g.b.b.b.p.r.d.b.b.b.b.d.b.b.b.b.b.,p.b.,p.).\\nCurrently, the NMR model was obtained from 2.3.4, 2.1, 7.2, 8.8.g.p.g.p.g.c.b.b.g.d.b.p.c.b.d.b.b.b.b'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_dialogue('Quantization is key', 0.8, 500, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T04:58:32.425322Z",
     "iopub.status.busy": "2023-11-17T04:58:32.424923Z",
     "iopub.status.idle": "2023-11-17T04:58:37.640953Z",
     "shell.execute_reply": "2023-11-17T04:58:37.639709Z",
     "shell.execute_reply.started": "2023-11-17T04:58:32.425293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Indian economy falls to its lowest in the country in the country.\\nSandy, Lauded Herald Griy, a member of Bono and Lima, with the Axianaian. The party was forced to have a very caucasiac. As a result, the government was adopted by\\nThe plan was presented by a group of leaders of the African Republic, where the U.S. Bank elected to their national community, although the number of which were for the sovereign interests and the Soviet Union (1970), so which it was not a major part of the national, rather than the election. It was a Romanian.\\nIt was a surprise to the support for the government in the US. The government was an irresponsible and in this place which the Houth Estrella regime was established in 1996 and 1929.\\nThe Gulf of Thailand was a crucial role in Europe in 1934 and 1955.\\nThe Jihan is an act of the United States and the United States and for his support of the United States and the United States, as it is also an important contribution to the foreign government.\\nThe United States government has moved on to the Armenian capital, and is also the President of the United States.\\nThe Indian government is a major effort which requires the national government to run the finals of the Republic by which the government has served and has been pursued by the Audi government.\\nAfghanistan has been named the first international economic opportunity to deal with the Islamic, soared and restless of their national security.\\nIt is also the key to the extent of the state and all of our government and the country.\\nTo sum up, it is to be considered a few from the states to be held in 1985 and 1996 for the Mentrainian and the provinces of the United States.\\nThe U.S. Bank in Turkey did not want the 1988-1700s and amend the largest and most significant region for the country.\\nThe U.S. Bank has built a strong increase in the country, but its value in the region, to the second possible period for the last years.\\nThe U.S. Bank has been forced to bring a majority of the nation’s population, including the 2'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_dialogue('Indian economy falls to its lowest', 0.8, 500, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T04:58:58.496851Z",
     "iopub.status.busy": "2023-11-17T04:58:58.496468Z",
     "iopub.status.idle": "2023-11-17T04:59:03.823828Z",
     "shell.execute_reply": "2023-11-17T04:59:03.822876Z",
     "shell.execute_reply.started": "2023-11-17T04:58:58.496822Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'American history is a dominant and an independent and a major role in the 1996-1991, the first time of the 1995-1990s, in the 1940s, was the first time in 1999.\\nThe 1970s, which is a group of 1979 and the oldest of the first 1960s, and has been a member of the National Association of the North American Academy of Sciences and the University of the United States.\\nThe 1970s, the University of the North American College of the University of Hawaii, and the University of the University of the University of the University of the University of New York.\\nThe 1970s, the University of the University of Pennsylvania, was the first time in 1991. In 1991, the University of Virginia, was the first time in 1994.\\nThe 1970s, the University of London, was the first time in 1999, and was the first time in 1994.\\nThe 1970s, which was a 1960s, which was then the first time in 1994, and was the first time in 1996.\\nThe 1970s, the 1970s, were the first time in 1995.\\nThe 1970s, 1970s, 1970s, 1970s, 1970s, 1970s, 1970s, 1970s, 1970s, 1970s, 1970s, 1970s, 1970s, 1970s, 1970s, 1970s, 1970s, 1970s, 1970s, 1970s, 1970s, 1970s, 1970s, 1970s, 1970s, 1970s, '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_dialogue('American history is a dominant', 0.4, 500, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T05:03:19.253982Z",
     "iopub.status.busy": "2023-11-17T05:03:19.253533Z",
     "iopub.status.idle": "2023-11-17T05:03:24.514166Z",
     "shell.execute_reply": "2023-11-17T05:03:24.512968Z",
     "shell.execute_reply.started": "2023-11-17T05:03:19.253950Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cartoons are kids way to enjoy television.\\nThe 2014-2013-1920\\nThe 2014-2013-2014\\nThe 1913-2012-2018\\nThe 1913-2014\\nThe 1914-2013 has been a number of 1.5 million people in the 1910s, including 13,000 people in the United States.\\nThe 1914-2019-2014-2019\\nThe 1914-2013-2019-2019-2019-2019-2019-1919-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-2019-'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_dialogue('Cartoons are kids way to enjoy television', 0.4, 500, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T05:04:22.827529Z",
     "iopub.status.busy": "2023-11-17T05:04:22.827054Z",
     "iopub.status.idle": "2023-11-17T05:04:28.051912Z",
     "shell.execute_reply": "2023-11-17T05:04:28.050792Z",
     "shell.execute_reply.started": "2023-11-17T05:04:22.827477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The galaxy we live in is of the community at the AML in Charter.\\nHowever, we have survived a more complex and outdoor of the home and the environment of the state and by the community than the government and the wake of God\\'s world.\\nLefebe said the future has been passed by the three chapters: I will be interested in a lot of the time and help teach these people who live in their spiritual struggle: it, and all I wish to have a more spiritual sense of being a very rich.\\nHe says the mission has been discussed again.\\nThe primary role that allows students to be a great opportunity to meet the arts, and then to say they\\'re a member of the Arts\\' Society, their families\\' work is currently reaping the use of the community.\\nI have passed, I need a scholarship at the school until 2011, now it has to develop a part of what I\\'m doing, and while I already receive a student will receive a whole ceremony and a local community to help communities open to the COVID-19 pandemic.\\nI also have a faculty from one of the major developments of the country, a school board program called the Newtown-led church entrance to England and all county students around the next and most importantly campuses. We have 1,000 students and 100 courses offer, and they\\'re all chosen in the 1850s and their all-agining facilities. We\\'ve always been serving the need for our best community and don\\'t have the option to make my experience trip!\\nA lot of them all-home bikes and I\\'m with a small student who provides a library for my experience.\\nI can\\'t use the BDS to take office a Sunday at the center of JP. The next session we had.\\nGet my first candy on your journey to the lone community again - we had my 3-0 words - but my guess was probably the same \"thank\" by my father, to one of these students, to another 50-year old.\\nBut now, we\\'re always seeking a church to see our 18-month old. We have a similar vintage plan and I think we can\\'t be putting together.\\nI also found some interesting results on our mission to visit the St. Thomas'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_dialogue('The galaxy we live in is', 0.9, 500, 200)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3808038,
     "sourceId": 6613614,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
