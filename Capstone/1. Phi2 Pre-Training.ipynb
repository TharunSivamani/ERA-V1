{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4ovHO8LDTWA",
        "outputId": "636a2c41-4477-4b1c-ee63-1a7261f322df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.7/777.7 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install lightning bitsandbytes --q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SmkaHwPI6k8s"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import lightning\n",
        "import bitsandbytes as bnb\n",
        "import gc\n",
        "import glob\n",
        "import math\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, Union\n",
        "import lightning as L\n",
        "import torch.nn as nn\n",
        "from lightning.fabric.loggers import CSVLogger\n",
        "from lightning.fabric.strategies import FSDPStrategy\n",
        "from torch.utils.data import DataLoader\n",
        "from tsai_gpt.model import GPT, Block, Config\n",
        "from tsai_gpt.packed_dataset import CombinedDataset, PackedDataset\n",
        "from tsai_gpt.speed_monitor import SpeedMonitorBase, estimate_flops, measure_flops\n",
        "from tsai_gpt.speed_monitor import SpeedMonitorFabric as SpeedMonitor\n",
        "from tsai_gpt.utils import chunked_cross_entropy, get_default_supported_precision, num_parameters, load_checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FSJ4kDcn6k8t"
      },
      "outputs": [],
      "source": [
        "# Configs\n",
        "model_name = \"phi-2\"\n",
        "name = \"phi-2\"\n",
        "out_dir = Path(\"out\") / name\n",
        "save_interval = 3000\n",
        "eval_interval = 3000\n",
        "eval_iters = 100\n",
        "log_interval = 3000\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 6e-3\n",
        "batch_size = 1\n",
        "micro_batch_size = 1\n",
        "gradient_accumulation_steps = 4\n",
        "assert gradient_accumulation_steps > 0\n",
        "max_iters = 3000\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0\n",
        "decay_lr = True\n",
        "warmup_iters = 2000\n",
        "lr_decay_iters = max_iters\n",
        "min_lr = 6e-6\n",
        "\n",
        "# Data configurations\n",
        "data_config = [\n",
        "    (\"c4\", 20.0),\n",
        "    (\"cc\", 80.0),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "P2vK2-xO6k8t"
      },
      "outputs": [],
      "source": [
        "hparams = {k: v for k, v in locals().items() if isinstance(v, (int, float, str)) and not k.startswith(\"_\")}\n",
        "logger = CSVLogger(\"out\", name, flush_logs_every_n_steps=log_interval)\n",
        "\n",
        "def setup(\n",
        "    devices: int = 4,\n",
        "    train_data_dir: Path = Path(\"data/capstone\"),\n",
        "    val_data_dir: Optional[Path] = None,\n",
        "    precision: Optional[str] = None,\n",
        "    resume: Union[bool, Path] = False,\n",
        ") -> None:\n",
        "    precision = precision or get_default_supported_precision(training=True)\n",
        "\n",
        "    if devices > 1:\n",
        "        strategy = FSDPStrategy(\n",
        "            auto_wrap_policy={Block},\n",
        "            activation_checkpointing_policy={Block},\n",
        "            state_dict_type=\"full\",\n",
        "            limit_all_gathers=True,\n",
        "            cpu_offload=False,\n",
        "        )\n",
        "    else:\n",
        "        strategy = \"auto\"\n",
        "\n",
        "    fabric = L.Fabric(devices=devices, strategy=strategy, precision=precision, loggers=logger)\n",
        "    fabric.print(hparams)\n",
        "    fabric.launch(main, train_data_dir, val_data_dir, resume)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vNtZq5gP6k8u"
      },
      "outputs": [],
      "source": [
        "def main(fabric: L.Fabric, train_data_dir: Path, val_data_dir: Path, resume: Union[bool, Path]) -> None:\n",
        "    global model_copy\n",
        "    speed_monitor = SpeedMonitor(fabric, window_size=50, time_unit=\"seconds\")\n",
        "\n",
        "    if fabric.global_rank == 0:\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    config = Config.from_name(model_name)\n",
        "\n",
        "    train_dataloader, val_dataloader = create_dataloaders(\n",
        "        batch_size=micro_batch_size,\n",
        "        block_size=config.block_size,\n",
        "        fabric=fabric,\n",
        "        train_data_dir=train_data_dir,\n",
        "        val_data_dir=val_data_dir,\n",
        "        seed = (1337 + fabric.global_rank),\n",
        "    )\n",
        "    if val_dataloader is None:\n",
        "        train_dataloader = fabric.setup_dataloaders(train_dataloader)\n",
        "    else:\n",
        "        train_dataloader, val_dataloader = fabric.setup_dataloaders(train_dataloader, val_dataloader)\n",
        "\n",
        "    fabric.seed_everything(1337)  # same seed for every process to init model (FSDP)\n",
        "    fabric.print(f\"Loading model with {config.__dict__}\")\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    def _init_weights(module: nn.Module) -> None:\n",
        "            \"\"\"Meant to be used with `gpt.apply(gpt._init_weights)`.\"\"\"\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "                if module.bias is not None:\n",
        "                    torch.nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    with fabric.init_module(empty_init=True):\n",
        "        model = GPT(config)\n",
        "        model.apply(_init_weights)\n",
        "    model.apply(_init_weights)\n",
        "\n",
        "    fabric.print(f\"Time to instantiate model: {time.perf_counter() - t0:.02f} seconds.\")\n",
        "    fabric.print(f\"Total parameters {num_parameters(model):,}\")\n",
        "\n",
        "    model = fabric.setup(model)\n",
        "    optimizer = bnb.optim.Adam8bit(\n",
        "        model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(beta1, beta2) #, foreach=False\n",
        "    )\n",
        "    optimizer = fabric.setup_optimizers(optimizer)\n",
        "    state = {\"model\": model, \"optimizer\": optimizer, \"hparams\": hparams, \"iter_num\": 0, \"step_count\": 0}\n",
        "\n",
        "    if resume is True:\n",
        "        resume = max(out_dir.glob(\"*.pth\"), key=lambda p: int(p.name.split(\"-\")[1]))\n",
        "    if resume:\n",
        "        fabric.print(f\"Resuming training from {resume}\")\n",
        "        fabric.load(resume, state)\n",
        "\n",
        "    train_time = time.perf_counter()\n",
        "    train(fabric, state, train_dataloader, val_dataloader, speed_monitor)\n",
        "    fabric.print(f\"Training time: {(time.perf_counter()-train_time):.2f}s\")\n",
        "    if fabric.device.type == \"cuda\":\n",
        "        fabric.print(f\"Memory used: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kMl2k2D76k8u"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    fabric: L.Fabric,\n",
        "    state: dict,\n",
        "    train_dataloader: DataLoader,\n",
        "    val_dataloader: DataLoader,\n",
        "    speed_monitor: SpeedMonitorBase,\n",
        ") -> None:\n",
        "    model     = state[\"model\"]\n",
        "    optimizer = state[\"optimizer\"]\n",
        "\n",
        "    if val_dataloader is not None:\n",
        "        validate(fabric, model, val_dataloader)  # sanity check\n",
        "\n",
        "    with torch.device(\"meta\"):\n",
        "        meta_model = GPT(model.config)\n",
        "\n",
        "        estimated_flops = estimate_flops(meta_model) * micro_batch_size\n",
        "        fabric.print(f\"Estimated TFLOPs: {estimated_flops * fabric.world_size / 1e12:.2f}\")\n",
        "        x = torch.randint(0, 1, (micro_batch_size, model.max_seq_length))\n",
        "        measured_flops = measure_flops(meta_model, x)\n",
        "        fabric.print(f\"Measured TFLOPs: {measured_flops * fabric.world_size / 1e12:.2f}\")\n",
        "        del meta_model, x\n",
        "\n",
        "    total_lengths = 0\n",
        "    total_t0 = time.perf_counter()\n",
        "\n",
        "    for state[\"iter_num\"], train_data in enumerate(train_dataloader, state[\"iter_num\"]):\n",
        "        if state[\"iter_num\"] >= max_iters:\n",
        "            checkpoint_path = out_dir / f\"iter-{state['iter_num']:06d}-ckpt.pth\"\n",
        "            fabric.print(f\"Saving checkpoint to {str(checkpoint_path)!r}\")\n",
        "            fabric.save(checkpoint_path, state)\n",
        "            break\n",
        "\n",
        "        lr = get_lr(state[\"iter_num\"]) if decay_lr else learning_rate\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group[\"lr\"] = lr\n",
        "\n",
        "        iter_t0 = time.perf_counter()\n",
        "\n",
        "        input_ids = train_data[:, 0 : model.max_seq_length].contiguous()\n",
        "        targets = train_data[:, 1 : model.max_seq_length + 1].contiguous()\n",
        "\n",
        "        is_accumulating = (state[\"iter_num\"] + 1) % gradient_accumulation_steps != 0\n",
        "        with fabric.no_backward_sync(model, enabled=is_accumulating):\n",
        "            logits = model(input_ids)\n",
        "            loss = chunked_cross_entropy(logits, targets, chunk_size=0)\n",
        "            fabric.backward(loss / gradient_accumulation_steps)\n",
        "\n",
        "        if not is_accumulating:\n",
        "            fabric.clip_gradients(model, optimizer, max_norm=grad_clip)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            state[\"step_count\"] += 1\n",
        "\n",
        "        t1 = time.perf_counter()\n",
        "        total_lengths += input_ids.size(1)\n",
        "        speed_monitor.on_train_batch_end(\n",
        "            (state[\"iter_num\"] + 1) * micro_batch_size,\n",
        "            t1 - total_t0,\n",
        "\n",
        "            fabric.world_size,\n",
        "            flops_per_batch=measured_flops,\n",
        "            lengths=total_lengths,\n",
        "        )\n",
        "        if state[\"iter_num\"] % log_interval == 0:\n",
        "            fabric.print(\n",
        "                f\"iter {state['iter_num']} step {state['step_count']}: loss {loss.item():.4f}, LR: {lr:.6f}, iter time:\"\n",
        "                f\" {(t1 - iter_t0) * 1000:.2f}ms{' (optimizer.step)' if not is_accumulating else ''}\"\n",
        "            )\n",
        "        if val_dataloader is not None and not is_accumulating and state[\"step_count\"] % eval_interval == 0:\n",
        "            t0 = time.perf_counter()\n",
        "            val_loss = validate(fabric, model, val_dataloader)\n",
        "            t1 = time.perf_counter() - t0\n",
        "            speed_monitor.eval_end(t1)\n",
        "            fabric.print(f\"step {state['iter_num']}: val loss {val_loss.item():.4f}, val time: {t1 * 1000:.2f}ms\")\n",
        "            fabric.barrier()\n",
        "        if not is_accumulating and state[\"step_count\"] % save_interval == 0:\n",
        "            checkpoint_path = out_dir / f\"iter-{state['iter_num']:06d}-ckpt.pth\"\n",
        "            fabric.print(f\"Saving checkpoint to {str(checkpoint_path)!r}\")\n",
        "            fabric.save(checkpoint_path, state)\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def validate(fabric: L.Fabric, model: torch.nn.Module, val_dataloader: DataLoader) -> torch.Tensor:\n",
        "    fabric.print(\"Validating ...\")\n",
        "    model.eval()\n",
        "\n",
        "    losses = torch.zeros(eval_iters, device=fabric.device)\n",
        "    for k, val_data in enumerate(val_dataloader):\n",
        "        input_ids = val_data[:, 0 : model.max_seq_length].contiguous()\n",
        "        targets = val_data[:, 1 : model.max_seq_length + 1].contiguous()\n",
        "        logits = model(input_ids)\n",
        "        losses[k] = chunked_cross_entropy(logits, targets, chunk_size=0)\n",
        "    out = losses.mean()\n",
        "\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "G7hFTpFk6k8v"
      },
      "outputs": [],
      "source": [
        "def create_dataloader(\n",
        "    batch_size: int, block_size: int, data_dir: Path, fabric: L.Fabric, shuffle: bool = True, seed: int = 12345\n",
        ") -> DataLoader:\n",
        "    datasets = []\n",
        "    for prefix, _ in data_config:\n",
        "        filenames = glob.glob(str(data_dir / f\"{prefix}*\"))\n",
        "        dataset = PackedDataset(\n",
        "            filenames,\n",
        "            n_chunks=1,\n",
        "            block_size=block_size,\n",
        "            shuffle=shuffle,\n",
        "            seed=seed,\n",
        "            num_processes=fabric.world_size,\n",
        "            process_rank=fabric.global_rank,\n",
        "        )\n",
        "        datasets.append(dataset)\n",
        "    if not datasets:\n",
        "        raise RuntimeError(\n",
        "            f\"No data found at {data_dir}. Make sure you ran prepare_redpajama.py to create the dataset.\"\n",
        "        )\n",
        "    weights = [weight for _, weight in data_config]\n",
        "    sum_weights = sum(weights)\n",
        "    weights = [el / sum_weights for el in weights]\n",
        "    combined_dataset = CombinedDataset(datasets=datasets, seed=seed, weights=weights)\n",
        "\n",
        "    return DataLoader(combined_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "\n",
        "def create_dataloaders(\n",
        "    batch_size: int,\n",
        "    block_size: int,\n",
        "    fabric: L.Fabric,\n",
        "    train_data_dir: Path = Path(\"data/sample\"),\n",
        "    val_data_dir: Optional[Path] = None,\n",
        "    seed: int = 12345,\n",
        ") -> Tuple[DataLoader, DataLoader]:\n",
        "\n",
        "    effective_block_size = block_size + 1\n",
        "    train_dataloader = create_dataloader(\n",
        "        batch_size=batch_size,\n",
        "        block_size=effective_block_size,\n",
        "        fabric=fabric,\n",
        "        data_dir=train_data_dir,\n",
        "        shuffle=True,\n",
        "        seed=seed,\n",
        "    )\n",
        "    val_dataloader = (\n",
        "        create_dataloader(\n",
        "            batch_size=batch_size,\n",
        "            block_size=effective_block_size,\n",
        "            fabric=fabric,\n",
        "            data_dir=val_data_dir,\n",
        "            shuffle=False,\n",
        "            seed=seed,\n",
        "        )\n",
        "        if val_data_dir\n",
        "        else None\n",
        "    )\n",
        "    return train_dataloader, val_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "keSWtdpO6k8v"
      },
      "outputs": [],
      "source": [
        "## LR scheduler\n",
        "def get_lr(it: int) -> float:\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "\n",
        "    return min_lr + coeff * (learning_rate - min_lr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.amp.autocast(enabled=True)\n",
        "\n",
        "setup(\n",
        "    devices=1,\n",
        "    train_data_dir=Path(\"data/sample100MB\")\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pybacZOZtq1u",
        "outputId": "f5321652-67cb-4bed-9751-8bd6acdaa39c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "INFO: Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "{'model_name': 'phi-2', 'name': 'phi-2', 'save_interval': 1000, 'eval_interval': 1000, 'eval_iters': 100, 'log_interval': 100, 'learning_rate': 0.006, 'batch_size': 1, 'micro_batch_size': 1, 'gradient_accumulation_steps': 4, 'max_iters': 10000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 2000, 'lr_decay_iters': 10000, 'min_lr': 6e-06}\n",
            "INFO: Seed set to 1337\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 1337\n",
            "Loading model with {'name': 'phi-2', 'hf_config': {'org': 'microsoft', 'name': 'phi-2'}, 'block_size': 256, 'vocab_size': 51200, 'padding_multiple': 512, 'padded_vocab_size': 51200, 'n_layer': 32, 'n_head': 32, 'n_embd': 2560, 'rotary_percentage': 0.4, 'parallel_residual': True, 'bias': True, 'lm_head_bias': True, 'n_query_groups': 32, 'shared_attention_norm': True, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'tanh', 'intermediate_size': 10240, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 80, 'rope_n_elem': 32}\n",
            "Time to instantiate model: 0.44 seconds.\n",
            "Total parameters 2,779,683,840\n",
            "Estimated TFLOPs: 4.38\n",
            "Measured TFLOPs: 4.13\n",
            "iter 0 step 0: loss 10.7340, LR: 0.000000, iter time: 1210.67ms\n",
            "iter 100 step 25: loss 7.9553, LR: 0.000300, iter time: 100.06ms\n",
            "iter 200 step 50: loss 7.9136, LR: 0.000600, iter time: 95.83ms\n",
            "iter 300 step 75: loss 8.9131, LR: 0.000900, iter time: 95.73ms\n",
            "iter 400 step 100: loss 9.0141, LR: 0.001200, iter time: 97.15ms\n",
            "iter 500 step 125: loss 8.8424, LR: 0.001500, iter time: 97.29ms\n",
            "iter 600 step 150: loss 9.1321, LR: 0.001800, iter time: 97.54ms\n",
            "iter 700 step 175: loss 7.5628, LR: 0.002100, iter time: 96.22ms\n",
            "iter 800 step 200: loss 8.1672, LR: 0.002400, iter time: 98.89ms\n",
            "iter 900 step 225: loss 7.8912, LR: 0.002700, iter time: 95.66ms\n",
            "iter 1000 step 250: loss 8.5327, LR: 0.003000, iter time: 97.23ms\n",
            "iter 1100 step 275: loss 6.1776, LR: 0.003300, iter time: 96.62ms\n",
            "iter 1200 step 300: loss 8.9131, LR: 0.003600, iter time: 93.58ms\n",
            "iter 1300 step 325: loss 7.9728, LR: 0.003900, iter time: 96.73ms\n",
            "iter 1400 step 350: loss 8.6512, LR: 0.004200, iter time: 95.93ms\n",
            "iter 1500 step 375: loss 8.5109, LR: 0.004500, iter time: 96.44ms\n",
            "iter 1600 step 400: loss 8.1521, LR: 0.004800, iter time: 94.96ms\n",
            "iter 1700 step 425: loss 8.5120, LR: 0.005100, iter time: 94.43ms\n",
            "iter 1800 step 450: loss 7.7462, LR: 0.005400, iter time: 96.84ms\n",
            "iter 1900 step 475: loss 7.4216, LR: 0.005700, iter time: 95.84ms\n",
            "iter 2000 step 500: loss 7.2980, LR: 0.006000, iter time: 91.55ms\n",
            "iter 2100 step 525: loss 8.5015, LR: 0.005998, iter time: 95.05ms\n",
            "iter 2200 step 550: loss 7.4835, LR: 0.005991, iter time: 96.98ms\n",
            "iter 2300 step 575: loss 7.5653, LR: 0.005979, iter time: 97.88ms\n",
            "iter 2400 step 600: loss 7.6941, LR: 0.005963, iter time: 96.96ms\n",
            "iter 2500 step 625: loss 8.0123, LR: 0.005942, iter time: 95.29ms\n",
            "iter 2600 step 650: loss 8.1154, LR: 0.005917, iter time: 96.71ms\n",
            "iter 2700 step 675: loss 6.5120, LR: 0.005887, iter time: 96.51ms\n",
            "iter 2800 step 700: loss 7.7218, LR: 0.005853, iter time: 96.25ms\n",
            "iter 2900 step 725: loss 7.4919, LR: 0.005815, iter time: 97.89ms\n",
            "iter 3000 step 750: loss 7.1346, LR: 0.005772, iter time: 97.59ms \n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}