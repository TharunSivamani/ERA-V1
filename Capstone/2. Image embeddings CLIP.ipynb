{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEkgM2Nyej0q",
        "outputId": "d90267f7-07da-42b4-f91a-69d09868c1cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m51.2/53.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install ftfy regex tqdm --q\n",
        "! pip install git+https://github.com/openai/CLIP.git --q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGOOs2PxenyU",
        "outputId": "bd329326-02df-4fa4-820e-2fcea1d487d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:01<00:00, 200MiB/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "\n",
        "# Load the model\n",
        "# Source - https://github.com/openai/CLIP\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cUaC93kLeppZ"
      },
      "outputs": [],
      "source": [
        "def encode_image(image_path):\n",
        "    image = Image.open(image_path)\n",
        "    image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Calculate features\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image_input)\n",
        "\n",
        "    return image_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OsllDa0vB1Nq"
      },
      "outputs": [],
      "source": [
        "image_embedding_dict = {}\n",
        "\n",
        "for root, dirs, files in os.walk(\"/content/coco2017/\"):   # traversing through all folders\n",
        "    for file in files:\n",
        "        if file.lower().endswith(('.jpg')):\n",
        "            image_path = os.path.join(root, file)\n",
        "            image_embedding = encode_image(image_path)    # encoding the image as embeddings\n",
        "            image_embedding_dict[file] = image_embedding  # storing the embeddings in a dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "is60S_VAmL65"
      },
      "outputs": [],
      "source": [
        "# saving the dict\n",
        "torch.save(image_embedding_dict, \"image_embedding_dict.pth\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
