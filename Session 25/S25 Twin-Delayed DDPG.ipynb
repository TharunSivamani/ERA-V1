{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXu1r8qvSzWf"
      },
      "source": [
        "# Twin-Delayed DDPG\n",
        "\n",
        "Complete credit goes to this [awesome Deep Reinforcement Learning 2.0 Course on Udemy](https://www.udemy.com/course/deep-reinforcement-learning/) for the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRzQUhuUTc0J"
      },
      "source": [
        "## Installing the packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAHMB0Ze8fU0",
        "outputId": "3d85a0bc-39e3-4ce3-b095-096433617c15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m631.1/631.1 kB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install pybullet gym==0.22 --q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjm2onHdT-Av"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ikr2p0Js8iB4",
        "outputId": "46db7386-9830-4a07-cb92-9350b4a399b9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nGdtlKVydr"
      },
      "source": [
        "## Step 1: We initialize the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u5rW0IDB8nTO"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind:\n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb7TTaHxWbQD"
      },
      "source": [
        "## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4CeRW4D79HL0"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRDDce8FXef7"
      },
      "source": [
        "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OCee7gwR9Jrs"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzIDuONodenW"
      },
      "source": [
        "## Steps 4 to 15: Training Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zzd0H1xukdKe"
      },
      "outputs": [],
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "\n",
        "    for it in range(iterations):\n",
        "\n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "\n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "\n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "\n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "\n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "\n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "\n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "\n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka-ZRtQvjBex"
      },
      "source": [
        "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qabqiYdp9wDM"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGuKmH_ijf7U"
      },
      "source": [
        "## We set the parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HFj6wbAo97lk"
      },
      "outputs": [],
      "source": [
        "env_name = \"AntBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjwf2HCol3XP"
      },
      "source": [
        "## We create a file name for the two saved models: the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fyH8N5z-o3o",
        "outputId": "629f0ceb-84fc-45bb-ec45-c76eb381a93b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ]
        }
      ],
      "source": [
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kop-C96Aml8O"
      },
      "source": [
        "## We create a folder inside which will be saved the trained models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Src07lvY-zXb"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEAzOd47mv1Z"
      },
      "source": [
        "## We create the PyBullet environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CyQXJUIs-6BV"
      },
      "outputs": [],
      "source": [
        "env = gym.make(env_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YdPG4HXnNsh"
      },
      "source": [
        "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Z3RufYec_ADj"
      },
      "outputs": [],
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01ydkALCb4DR",
        "outputId": "c8e82304-7c49-461b-9b8f-8e2e47462b8e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(28, 8, 1.0)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state_dim, action_dim, max_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWEgDAQxnbem"
      },
      "source": [
        "## We create the policy network (the Actor model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wTVvG7F8_EWg"
      },
      "outputs": [],
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI60VN2Unklh"
      },
      "source": [
        "## We create the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sd-ZsdXR_LgV"
      },
      "outputs": [],
      "source": [
        "replay_buffer = ReplayBuffer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOpCyiDnw7s"
      },
      "source": [
        "## We define a list where all the evaluation results over 10 episodes are stored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhC_5XJ__Orp",
        "outputId": "f288da3f-a41f-40de-93f3-7978dca87634"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 9.807990\n",
            "---------------------------------------\n"
          ]
        }
      ],
      "source": [
        "evaluations = [evaluate_policy(policy)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm-4b3p6rglE"
      },
      "source": [
        "## We create a new folder directory in which the final results (videos of the agent) will be populated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MTL9uMd0ru03"
      },
      "outputs": [],
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31n5eb03p-Fm"
      },
      "source": [
        "## We initialize the variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1vN5EvxK_QhT"
      },
      "outputs": [],
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9gsjvtPqLgT"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_ouY4NH_Y0I",
        "outputId": "857f9cbc-453a-4aab-ac8b-66ebe3aa0155"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Timesteps: 1000 Episode Num: 1 Reward: 505.82637647263067\n",
            "Total Timesteps: 2000 Episode Num: 2 Reward: 414.6351676537608\n",
            "Total Timesteps: 3000 Episode Num: 3 Reward: 489.50846108094237\n",
            "Total Timesteps: 3422 Episode Num: 4 Reward: 202.6144721972363\n",
            "Total Timesteps: 3543 Episode Num: 5 Reward: 46.49724536909923\n",
            "Total Timesteps: 3702 Episode Num: 6 Reward: 74.94773442125732\n",
            "Total Timesteps: 4702 Episode Num: 7 Reward: 454.577185863701\n",
            "Total Timesteps: 5702 Episode Num: 8 Reward: 466.5880697709991\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 164.590321\n",
            "---------------------------------------\n",
            "Total Timesteps: 5993 Episode Num: 9 Reward: 139.26799699744157\n",
            "Total Timesteps: 6660 Episode Num: 10 Reward: 306.9155873793611\n",
            "Total Timesteps: 7660 Episode Num: 11 Reward: 453.55892317350396\n",
            "Total Timesteps: 8660 Episode Num: 12 Reward: 480.3211801743045\n",
            "Total Timesteps: 9569 Episode Num: 13 Reward: 505.87623810379966\n",
            "Total Timesteps: 10569 Episode Num: 14 Reward: 412.06511016005504\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 263.067843\n",
            "---------------------------------------\n",
            "Total Timesteps: 11569 Episode Num: 15 Reward: 249.71248732765113\n",
            "Total Timesteps: 12569 Episode Num: 16 Reward: 197.0165414090558\n",
            "Total Timesteps: 12886 Episode Num: 17 Reward: 12.626860547346725\n",
            "Total Timesteps: 13886 Episode Num: 18 Reward: 99.56271082053445\n",
            "Total Timesteps: 14886 Episode Num: 19 Reward: 351.29539389650984\n",
            "Total Timesteps: 15886 Episode Num: 20 Reward: 619.9599473881622\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 85.365003\n",
            "---------------------------------------\n",
            "Total Timesteps: 16226 Episode Num: 21 Reward: 36.92094094350413\n",
            "Total Timesteps: 17226 Episode Num: 22 Reward: 368.4148256045914\n",
            "Total Timesteps: 18226 Episode Num: 23 Reward: 424.9897183891468\n",
            "Total Timesteps: 19226 Episode Num: 24 Reward: 447.4667131998552\n",
            "Total Timesteps: 19246 Episode Num: 25 Reward: 1.4459133114485216\n",
            "Total Timesteps: 19641 Episode Num: 26 Reward: 112.18443915684799\n",
            "Total Timesteps: 20641 Episode Num: 27 Reward: 247.2493099296605\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 451.249140\n",
            "---------------------------------------\n",
            "Total Timesteps: 21641 Episode Num: 28 Reward: 391.04242800792105\n",
            "Total Timesteps: 21957 Episode Num: 29 Reward: 176.21727512667513\n",
            "Total Timesteps: 22957 Episode Num: 30 Reward: 534.414576655491\n",
            "Total Timesteps: 23957 Episode Num: 31 Reward: 419.15344526034625\n",
            "Total Timesteps: 24957 Episode Num: 32 Reward: 541.2316200154745\n",
            "Total Timesteps: 25957 Episode Num: 33 Reward: 317.1566640862111\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 530.078171\n",
            "---------------------------------------\n",
            "Total Timesteps: 26957 Episode Num: 34 Reward: 527.4448634862893\n",
            "Total Timesteps: 27957 Episode Num: 35 Reward: 479.1999931418346\n",
            "Total Timesteps: 28957 Episode Num: 36 Reward: 353.56088190631505\n",
            "Total Timesteps: 29957 Episode Num: 37 Reward: 306.42105442187943\n",
            "Total Timesteps: 30957 Episode Num: 38 Reward: 444.76589565993766\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 380.238539\n",
            "---------------------------------------\n",
            "Total Timesteps: 31957 Episode Num: 39 Reward: 369.6730963367578\n",
            "Total Timesteps: 32957 Episode Num: 40 Reward: 290.46575902805137\n",
            "Total Timesteps: 33957 Episode Num: 41 Reward: 212.14050398524617\n",
            "Total Timesteps: 34957 Episode Num: 42 Reward: 395.25929591466456\n",
            "Total Timesteps: 35957 Episode Num: 43 Reward: 342.879980579242\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 361.752290\n",
            "---------------------------------------\n",
            "Total Timesteps: 36957 Episode Num: 44 Reward: 443.47393842667594\n",
            "Total Timesteps: 37957 Episode Num: 45 Reward: 514.9225178994596\n",
            "Total Timesteps: 38957 Episode Num: 46 Reward: 264.2830259501733\n",
            "Total Timesteps: 39957 Episode Num: 47 Reward: 328.6642446264696\n",
            "Total Timesteps: 40957 Episode Num: 48 Reward: 372.8733955947883\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 367.600867\n",
            "---------------------------------------\n",
            "Total Timesteps: 41957 Episode Num: 49 Reward: 423.71284055491276\n",
            "Total Timesteps: 42957 Episode Num: 50 Reward: 324.1087063182196\n",
            "Total Timesteps: 43957 Episode Num: 51 Reward: 330.34784098192904\n",
            "Total Timesteps: 44957 Episode Num: 52 Reward: 215.5091653113805\n",
            "Total Timesteps: 45957 Episode Num: 53 Reward: 395.110036707238\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 214.225767\n",
            "---------------------------------------\n",
            "Total Timesteps: 46957 Episode Num: 54 Reward: 249.74889864385997\n",
            "Total Timesteps: 47957 Episode Num: 55 Reward: 284.2248748372734\n",
            "Total Timesteps: 48165 Episode Num: 56 Reward: 36.850283873086035\n",
            "Total Timesteps: 48229 Episode Num: 57 Reward: 17.46407920016664\n",
            "Total Timesteps: 49229 Episode Num: 58 Reward: 390.29893448434666\n",
            "Total Timesteps: 50229 Episode Num: 59 Reward: 235.67333035972416\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 289.393173\n",
            "---------------------------------------\n",
            "Total Timesteps: 51229 Episode Num: 60 Reward: 132.1569742978705\n",
            "Total Timesteps: 52229 Episode Num: 61 Reward: 587.0071412800406\n",
            "Total Timesteps: 53229 Episode Num: 62 Reward: 433.363438609196\n",
            "Total Timesteps: 54229 Episode Num: 63 Reward: 514.8477591239829\n",
            "Total Timesteps: 55229 Episode Num: 64 Reward: 539.2762879524419\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 590.378099\n",
            "---------------------------------------\n",
            "Total Timesteps: 56229 Episode Num: 65 Reward: 480.3304293023475\n",
            "Total Timesteps: 57229 Episode Num: 66 Reward: 735.0495783497475\n",
            "Total Timesteps: 58229 Episode Num: 67 Reward: 335.81906866572604\n",
            "Total Timesteps: 59229 Episode Num: 68 Reward: 519.6157413086725\n",
            "Total Timesteps: 60229 Episode Num: 69 Reward: 500.4173253338382\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 555.915113\n",
            "---------------------------------------\n",
            "Total Timesteps: 61229 Episode Num: 70 Reward: 437.0499190046772\n",
            "Total Timesteps: 62229 Episode Num: 71 Reward: 356.25880835171023\n",
            "Total Timesteps: 63229 Episode Num: 72 Reward: 544.4171560365045\n",
            "Total Timesteps: 64229 Episode Num: 73 Reward: 559.2728636037956\n",
            "Total Timesteps: 65229 Episode Num: 74 Reward: 783.4220726204459\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 464.547603\n",
            "---------------------------------------\n",
            "Total Timesteps: 66229 Episode Num: 75 Reward: 427.5230054365948\n",
            "Total Timesteps: 67229 Episode Num: 76 Reward: 405.126560495765\n",
            "Total Timesteps: 68229 Episode Num: 77 Reward: 491.0616994009456\n",
            "Total Timesteps: 69229 Episode Num: 78 Reward: 417.59925676327634\n",
            "Total Timesteps: 70229 Episode Num: 79 Reward: 547.961333769364\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 8.820522\n",
            "---------------------------------------\n",
            "Total Timesteps: 70249 Episode Num: 80 Reward: 3.617803361068733\n",
            "Total Timesteps: 70269 Episode Num: 81 Reward: 3.450396436791092\n",
            "Total Timesteps: 71269 Episode Num: 82 Reward: 547.8877051523684\n",
            "Total Timesteps: 72269 Episode Num: 83 Reward: 537.8408090074493\n",
            "Total Timesteps: 73269 Episode Num: 84 Reward: 457.44467001144716\n",
            "Total Timesteps: 74269 Episode Num: 85 Reward: 246.073843346709\n",
            "Total Timesteps: 74705 Episode Num: 86 Reward: 225.08701443708736\n",
            "Total Timesteps: 75705 Episode Num: 87 Reward: 589.3005197346596\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 331.014743\n",
            "---------------------------------------\n",
            "Total Timesteps: 75746 Episode Num: 88 Reward: 16.996321861110246\n",
            "Total Timesteps: 76746 Episode Num: 89 Reward: 591.1229806446366\n",
            "Total Timesteps: 77746 Episode Num: 90 Reward: 515.0444682571153\n",
            "Total Timesteps: 78746 Episode Num: 91 Reward: 561.966865426163\n",
            "Total Timesteps: 79194 Episode Num: 92 Reward: 275.714384817327\n",
            "Total Timesteps: 80194 Episode Num: 93 Reward: 406.323586143015\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 228.740167\n",
            "---------------------------------------\n",
            "Total Timesteps: 80389 Episode Num: 94 Reward: 111.72951085225834\n",
            "Total Timesteps: 81336 Episode Num: 95 Reward: 458.09508402224066\n",
            "Total Timesteps: 82336 Episode Num: 96 Reward: 452.54139698966804\n",
            "Total Timesteps: 83336 Episode Num: 97 Reward: 472.28836776160654\n",
            "Total Timesteps: 84336 Episode Num: 98 Reward: 585.2508352634877\n",
            "Total Timesteps: 85336 Episode Num: 99 Reward: 522.558662465101\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 366.252209\n",
            "---------------------------------------\n",
            "Total Timesteps: 86336 Episode Num: 100 Reward: 521.5963251649839\n",
            "Total Timesteps: 87336 Episode Num: 101 Reward: 529.3338022957133\n",
            "Total Timesteps: 88336 Episode Num: 102 Reward: 564.0059358623324\n",
            "Total Timesteps: 89336 Episode Num: 103 Reward: 548.4883158783732\n",
            "Total Timesteps: 90336 Episode Num: 104 Reward: 450.40347274166953\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 512.549840\n",
            "---------------------------------------\n",
            "Total Timesteps: 91336 Episode Num: 105 Reward: 258.3589318429541\n",
            "Total Timesteps: 92336 Episode Num: 106 Reward: 600.5670623977173\n",
            "Total Timesteps: 93336 Episode Num: 107 Reward: 593.7381141945533\n",
            "Total Timesteps: 93944 Episode Num: 108 Reward: 346.438704716896\n",
            "Total Timesteps: 94944 Episode Num: 109 Reward: 395.66259030363943\n",
            "Total Timesteps: 95944 Episode Num: 110 Reward: 254.7484598846433\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 12.034091\n",
            "---------------------------------------\n",
            "Total Timesteps: 96064 Episode Num: 111 Reward: 54.62328869099397\n",
            "Total Timesteps: 96266 Episode Num: 112 Reward: 82.74504317674612\n",
            "Total Timesteps: 97266 Episode Num: 113 Reward: 485.10983994516295\n",
            "Total Timesteps: 98266 Episode Num: 114 Reward: 406.0461176600053\n",
            "Total Timesteps: 99266 Episode Num: 115 Reward: 437.2477345597879\n",
            "Total Timesteps: 100266 Episode Num: 116 Reward: 317.8450934077534\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 372.058458\n",
            "---------------------------------------\n",
            "Total Timesteps: 101266 Episode Num: 117 Reward: 361.2049727860162\n",
            "Total Timesteps: 102266 Episode Num: 118 Reward: 322.28676406216863\n",
            "Total Timesteps: 103266 Episode Num: 119 Reward: 490.4949077735702\n",
            "Total Timesteps: 103427 Episode Num: 120 Reward: -34.39497757545328\n",
            "Total Timesteps: 104427 Episode Num: 121 Reward: 623.0697336787633\n",
            "Total Timesteps: 105427 Episode Num: 122 Reward: 458.23509034643104\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 139.923954\n",
            "---------------------------------------\n",
            "Total Timesteps: 106427 Episode Num: 123 Reward: 107.20794308322141\n",
            "Total Timesteps: 107427 Episode Num: 124 Reward: 299.37141896831577\n",
            "Total Timesteps: 108427 Episode Num: 125 Reward: 247.10897540028355\n",
            "Total Timesteps: 109427 Episode Num: 126 Reward: 253.24895975998282\n",
            "Total Timesteps: 110427 Episode Num: 127 Reward: 538.108300026817\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 445.695260\n",
            "---------------------------------------\n",
            "Total Timesteps: 111427 Episode Num: 128 Reward: 432.229190794384\n",
            "Total Timesteps: 112427 Episode Num: 129 Reward: 349.10646288136155\n",
            "Total Timesteps: 113427 Episode Num: 130 Reward: 751.427746522877\n",
            "Total Timesteps: 114427 Episode Num: 131 Reward: 384.43746180892254\n",
            "Total Timesteps: 114647 Episode Num: 132 Reward: 97.22067555275534\n",
            "Total Timesteps: 115647 Episode Num: 133 Reward: 685.5467190863724\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 534.929377\n",
            "---------------------------------------\n",
            "Total Timesteps: 116647 Episode Num: 134 Reward: 461.2790618438959\n",
            "Total Timesteps: 117647 Episode Num: 135 Reward: 435.70978729982727\n",
            "Total Timesteps: 118647 Episode Num: 136 Reward: 326.5375085146501\n",
            "Total Timesteps: 119647 Episode Num: 137 Reward: 498.93605526063374\n",
            "Total Timesteps: 120647 Episode Num: 138 Reward: 562.6085258221938\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 159.957044\n",
            "---------------------------------------\n",
            "Total Timesteps: 120825 Episode Num: 139 Reward: 49.18210942885036\n",
            "Total Timesteps: 121825 Episode Num: 140 Reward: 426.12736168268856\n",
            "Total Timesteps: 121874 Episode Num: 141 Reward: 12.030064946471786\n",
            "Total Timesteps: 122874 Episode Num: 142 Reward: 330.57508538519284\n",
            "Total Timesteps: 123874 Episode Num: 143 Reward: 445.33418677852893\n",
            "Total Timesteps: 124874 Episode Num: 144 Reward: 552.7897392920236\n",
            "Total Timesteps: 125874 Episode Num: 145 Reward: 473.750572180104\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 545.011487\n",
            "---------------------------------------\n",
            "Total Timesteps: 126874 Episode Num: 146 Reward: 528.2741954825027\n",
            "Total Timesteps: 127874 Episode Num: 147 Reward: 493.3153238840679\n",
            "Total Timesteps: 128874 Episode Num: 148 Reward: 617.4736455216429\n",
            "Total Timesteps: 128952 Episode Num: 149 Reward: 19.67187259594962\n",
            "Total Timesteps: 129003 Episode Num: 150 Reward: 21.865706516159754\n",
            "Total Timesteps: 129034 Episode Num: 151 Reward: -2.792987510848844\n",
            "Total Timesteps: 129067 Episode Num: 152 Reward: -4.091703371933459\n",
            "Total Timesteps: 129094 Episode Num: 153 Reward: -5.361862449438972\n",
            "Total Timesteps: 129128 Episode Num: 154 Reward: -1.556147698635796\n",
            "Total Timesteps: 129165 Episode Num: 155 Reward: 0.9507963697921222\n",
            "Total Timesteps: 129200 Episode Num: 156 Reward: -1.494309344704922\n",
            "Total Timesteps: 130200 Episode Num: 157 Reward: 561.808165650519\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 506.356277\n",
            "---------------------------------------\n",
            "Total Timesteps: 131200 Episode Num: 158 Reward: 501.68806281747516\n",
            "Total Timesteps: 132200 Episode Num: 159 Reward: 566.4197454050525\n",
            "Total Timesteps: 133200 Episode Num: 160 Reward: 368.3038504688687\n",
            "Total Timesteps: 134200 Episode Num: 161 Reward: 542.477415556967\n",
            "Total Timesteps: 135200 Episode Num: 162 Reward: 301.2993170154665\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 501.218851\n",
            "---------------------------------------\n",
            "Total Timesteps: 136200 Episode Num: 163 Reward: 529.3687539934901\n",
            "Total Timesteps: 137200 Episode Num: 164 Reward: 504.0902472384883\n",
            "Total Timesteps: 138200 Episode Num: 165 Reward: 439.6007576486948\n",
            "Total Timesteps: 139200 Episode Num: 166 Reward: 637.7452359959356\n",
            "Total Timesteps: 140200 Episode Num: 167 Reward: 391.10327450189084\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 478.507127\n",
            "---------------------------------------\n",
            "Total Timesteps: 141200 Episode Num: 168 Reward: 542.8658395169631\n",
            "Total Timesteps: 142200 Episode Num: 169 Reward: 523.0769080806598\n",
            "Total Timesteps: 142241 Episode Num: 170 Reward: 21.74166180681589\n",
            "Total Timesteps: 142290 Episode Num: 171 Reward: 20.47535289679309\n",
            "Total Timesteps: 142358 Episode Num: 172 Reward: 30.927096596129385\n",
            "Total Timesteps: 143358 Episode Num: 173 Reward: 503.6272509801002\n",
            "Total Timesteps: 143930 Episode Num: 174 Reward: 403.3223415428659\n",
            "Total Timesteps: 144930 Episode Num: 175 Reward: 635.9646992420763\n",
            "Total Timesteps: 145930 Episode Num: 176 Reward: 539.8125162016396\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 495.890475\n",
            "---------------------------------------\n",
            "Total Timesteps: 146930 Episode Num: 177 Reward: 612.8970664312054\n",
            "Total Timesteps: 147930 Episode Num: 178 Reward: 609.2559879341518\n",
            "Total Timesteps: 148930 Episode Num: 179 Reward: 728.0770382708238\n",
            "Total Timesteps: 149930 Episode Num: 180 Reward: 790.8057349608346\n",
            "Total Timesteps: 150930 Episode Num: 181 Reward: 469.8826164840749\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 627.769964\n",
            "---------------------------------------\n",
            "Total Timesteps: 151930 Episode Num: 182 Reward: 513.9773220461058\n",
            "Total Timesteps: 152930 Episode Num: 183 Reward: 687.1535829475616\n",
            "Total Timesteps: 153930 Episode Num: 184 Reward: 734.709795444576\n",
            "Total Timesteps: 154930 Episode Num: 185 Reward: 526.8945488731968\n",
            "Total Timesteps: 155837 Episode Num: 186 Reward: 395.7435091209696\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 431.712039\n",
            "---------------------------------------\n",
            "Total Timesteps: 156837 Episode Num: 187 Reward: 354.7654093807171\n",
            "Total Timesteps: 157493 Episode Num: 188 Reward: 262.5964236753531\n",
            "Total Timesteps: 158493 Episode Num: 189 Reward: 433.51289204656564\n",
            "Total Timesteps: 159493 Episode Num: 190 Reward: 317.0685002120178\n",
            "Total Timesteps: 160493 Episode Num: 191 Reward: 456.2325811489925\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 439.044488\n",
            "---------------------------------------\n",
            "Total Timesteps: 161493 Episode Num: 192 Reward: 667.5583026216777\n",
            "Total Timesteps: 162493 Episode Num: 193 Reward: 268.7560235201153\n",
            "Total Timesteps: 163493 Episode Num: 194 Reward: 606.3530595942402\n",
            "Total Timesteps: 164493 Episode Num: 195 Reward: 631.7600549952056\n",
            "Total Timesteps: 165493 Episode Num: 196 Reward: 504.4308862687841\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 539.747066\n",
            "---------------------------------------\n",
            "Total Timesteps: 166493 Episode Num: 197 Reward: 402.76343447583133\n",
            "Total Timesteps: 167493 Episode Num: 198 Reward: 536.0883639128408\n",
            "Total Timesteps: 168493 Episode Num: 199 Reward: 295.1216462894857\n",
            "Total Timesteps: 169493 Episode Num: 200 Reward: 545.5601210579131\n",
            "Total Timesteps: 170493 Episode Num: 201 Reward: 697.5890434742679\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 486.354975\n",
            "---------------------------------------\n",
            "Total Timesteps: 171493 Episode Num: 202 Reward: 554.6598169847683\n",
            "Total Timesteps: 172493 Episode Num: 203 Reward: 686.4918040339559\n",
            "Total Timesteps: 173493 Episode Num: 204 Reward: 531.7949108564042\n",
            "Total Timesteps: 174493 Episode Num: 205 Reward: 439.08179410816405\n",
            "Total Timesteps: 175493 Episode Num: 206 Reward: 475.29382605842983\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 544.013666\n",
            "---------------------------------------\n",
            "Total Timesteps: 176493 Episode Num: 207 Reward: 572.0851673432229\n",
            "Total Timesteps: 177493 Episode Num: 208 Reward: 611.4804367678065\n",
            "Total Timesteps: 178493 Episode Num: 209 Reward: 545.1913682711156\n",
            "Total Timesteps: 179493 Episode Num: 210 Reward: 858.278130899882\n",
            "Total Timesteps: 180493 Episode Num: 211 Reward: 514.3981982408066\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 609.933261\n",
            "---------------------------------------\n",
            "Total Timesteps: 181493 Episode Num: 212 Reward: 497.6606598991421\n",
            "Total Timesteps: 182493 Episode Num: 213 Reward: 716.881913991801\n",
            "Total Timesteps: 183493 Episode Num: 214 Reward: 623.6814470124799\n",
            "Total Timesteps: 184493 Episode Num: 215 Reward: 585.3811662855367\n",
            "Total Timesteps: 185493 Episode Num: 216 Reward: 535.0684780014393\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 506.822073\n",
            "---------------------------------------\n",
            "Total Timesteps: 186493 Episode Num: 217 Reward: 321.24208064968377\n",
            "Total Timesteps: 187493 Episode Num: 218 Reward: 612.2953150262276\n",
            "Total Timesteps: 188493 Episode Num: 219 Reward: 500.9540911143704\n",
            "Total Timesteps: 189493 Episode Num: 220 Reward: 674.7628163001725\n",
            "Total Timesteps: 190493 Episode Num: 221 Reward: 715.6967795465032\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 572.587402\n",
            "---------------------------------------\n",
            "Total Timesteps: 191493 Episode Num: 222 Reward: 662.8201219923586\n",
            "Total Timesteps: 192493 Episode Num: 223 Reward: 544.4349187797661\n",
            "Total Timesteps: 193493 Episode Num: 224 Reward: 644.0012025431535\n",
            "Total Timesteps: 194493 Episode Num: 225 Reward: 690.82976363877\n",
            "Total Timesteps: 195493 Episode Num: 226 Reward: 584.0462528925235\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 630.867175\n",
            "---------------------------------------\n",
            "Total Timesteps: 196493 Episode Num: 227 Reward: 713.4542795900632\n",
            "Total Timesteps: 197493 Episode Num: 228 Reward: 571.9659835658202\n",
            "Total Timesteps: 198493 Episode Num: 229 Reward: 424.2113273263112\n",
            "Total Timesteps: 199493 Episode Num: 230 Reward: 620.4816461352284\n",
            "Total Timesteps: 200493 Episode Num: 231 Reward: 728.8143967177483\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 588.267829\n",
            "---------------------------------------\n",
            "Total Timesteps: 201493 Episode Num: 232 Reward: 346.4567926663289\n",
            "Total Timesteps: 202493 Episode Num: 233 Reward: 360.58109040443156\n",
            "Total Timesteps: 203493 Episode Num: 234 Reward: 306.303130345854\n",
            "Total Timesteps: 204493 Episode Num: 235 Reward: 655.4319109142186\n",
            "Total Timesteps: 205493 Episode Num: 236 Reward: 495.76979081966925\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 691.106014\n",
            "---------------------------------------\n",
            "Total Timesteps: 206493 Episode Num: 237 Reward: 310.64206181068084\n",
            "Total Timesteps: 207493 Episode Num: 238 Reward: 754.0065961406989\n",
            "Total Timesteps: 208493 Episode Num: 239 Reward: 543.0261159165518\n",
            "Total Timesteps: 209493 Episode Num: 240 Reward: 538.9819536802461\n",
            "Total Timesteps: 210493 Episode Num: 241 Reward: 357.76463201945023\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 534.380874\n",
            "---------------------------------------\n",
            "Total Timesteps: 211493 Episode Num: 242 Reward: 522.5020053711141\n",
            "Total Timesteps: 212493 Episode Num: 243 Reward: 662.6551719577938\n",
            "Total Timesteps: 213493 Episode Num: 244 Reward: 623.4066045925482\n",
            "Total Timesteps: 214493 Episode Num: 245 Reward: 642.6728586621276\n",
            "Total Timesteps: 215493 Episode Num: 246 Reward: 632.7350260482655\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 575.033046\n",
            "---------------------------------------\n",
            "Total Timesteps: 216493 Episode Num: 247 Reward: 613.8290697074692\n",
            "Total Timesteps: 217493 Episode Num: 248 Reward: 712.5921711832459\n",
            "Total Timesteps: 218493 Episode Num: 249 Reward: 710.3512788215745\n",
            "Total Timesteps: 219493 Episode Num: 250 Reward: 745.6654284865795\n",
            "Total Timesteps: 220493 Episode Num: 251 Reward: 560.0469403975516\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 624.595749\n",
            "---------------------------------------\n",
            "Total Timesteps: 221493 Episode Num: 252 Reward: 431.9998760216461\n",
            "Total Timesteps: 222493 Episode Num: 253 Reward: 545.7096790482572\n",
            "Total Timesteps: 223493 Episode Num: 254 Reward: 459.3976860861393\n",
            "Total Timesteps: 224493 Episode Num: 255 Reward: 468.77770199591606\n",
            "Total Timesteps: 225493 Episode Num: 256 Reward: 728.7273121744252\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 600.431618\n",
            "---------------------------------------\n",
            "Total Timesteps: 226493 Episode Num: 257 Reward: 516.8349361062034\n",
            "Total Timesteps: 227493 Episode Num: 258 Reward: 668.7758689448495\n",
            "Total Timesteps: 228493 Episode Num: 259 Reward: 532.3826459115761\n",
            "Total Timesteps: 229493 Episode Num: 260 Reward: 695.1380862020404\n",
            "Total Timesteps: 230493 Episode Num: 261 Reward: 728.7468781819007\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 707.522658\n",
            "---------------------------------------\n",
            "Total Timesteps: 231493 Episode Num: 262 Reward: 506.6564460005586\n",
            "Total Timesteps: 232493 Episode Num: 263 Reward: 723.3303772154054\n",
            "Total Timesteps: 233493 Episode Num: 264 Reward: 632.6902361648532\n",
            "Total Timesteps: 234493 Episode Num: 265 Reward: 615.3448226403991\n",
            "Total Timesteps: 235493 Episode Num: 266 Reward: 645.9192365214883\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 692.510262\n",
            "---------------------------------------\n",
            "Total Timesteps: 236493 Episode Num: 267 Reward: 760.3499984585718\n",
            "Total Timesteps: 237493 Episode Num: 268 Reward: 637.5565153701126\n",
            "Total Timesteps: 238493 Episode Num: 269 Reward: 605.4747169067624\n",
            "Total Timesteps: 239493 Episode Num: 270 Reward: 798.5483205665204\n",
            "Total Timesteps: 240493 Episode Num: 271 Reward: 632.8119255626876\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 670.296285\n",
            "---------------------------------------\n",
            "Total Timesteps: 241493 Episode Num: 272 Reward: 749.1613720452601\n",
            "Total Timesteps: 242493 Episode Num: 273 Reward: 518.3450634233053\n",
            "Total Timesteps: 243493 Episode Num: 274 Reward: 732.3318364119613\n",
            "Total Timesteps: 244493 Episode Num: 275 Reward: 549.1573007738715\n",
            "Total Timesteps: 245493 Episode Num: 276 Reward: 718.3451455960596\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 742.000477\n",
            "---------------------------------------\n",
            "Total Timesteps: 246493 Episode Num: 277 Reward: 682.801344598744\n",
            "Total Timesteps: 247493 Episode Num: 278 Reward: 848.3861415688714\n",
            "Total Timesteps: 248493 Episode Num: 279 Reward: 675.3832058861482\n",
            "Total Timesteps: 249493 Episode Num: 280 Reward: 728.7932554973177\n",
            "Total Timesteps: 250493 Episode Num: 281 Reward: 643.5934826439161\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 727.447868\n",
            "---------------------------------------\n",
            "Total Timesteps: 251493 Episode Num: 282 Reward: 821.7294427590406\n",
            "Total Timesteps: 252493 Episode Num: 283 Reward: 822.207006822528\n",
            "Total Timesteps: 253493 Episode Num: 284 Reward: 853.8571421943961\n",
            "Total Timesteps: 254493 Episode Num: 285 Reward: 744.367663650415\n",
            "Total Timesteps: 255493 Episode Num: 286 Reward: 628.3421456133403\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 709.433289\n",
            "---------------------------------------\n",
            "Total Timesteps: 256493 Episode Num: 287 Reward: 623.7206075859236\n",
            "Total Timesteps: 257493 Episode Num: 288 Reward: 646.865406055509\n",
            "Total Timesteps: 258493 Episode Num: 289 Reward: 620.6268826188225\n",
            "Total Timesteps: 259493 Episode Num: 290 Reward: 459.7462522451713\n",
            "Total Timesteps: 260493 Episode Num: 291 Reward: 515.2927950494997\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 544.689589\n",
            "---------------------------------------\n",
            "Total Timesteps: 261493 Episode Num: 292 Reward: 487.4271888383816\n",
            "Total Timesteps: 262493 Episode Num: 293 Reward: 528.9541839984506\n",
            "Total Timesteps: 263493 Episode Num: 294 Reward: 528.6748149800999\n",
            "Total Timesteps: 264493 Episode Num: 295 Reward: 535.7129851517292\n",
            "Total Timesteps: 265493 Episode Num: 296 Reward: 365.7126746672397\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 402.737788\n",
            "---------------------------------------\n",
            "Total Timesteps: 266493 Episode Num: 297 Reward: 278.36707656277366\n",
            "Total Timesteps: 267493 Episode Num: 298 Reward: 427.3746367926906\n",
            "Total Timesteps: 268493 Episode Num: 299 Reward: 582.8387395589858\n",
            "Total Timesteps: 269493 Episode Num: 300 Reward: 560.6629899273071\n",
            "Total Timesteps: 270493 Episode Num: 301 Reward: 523.3830154605473\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 618.230082\n",
            "---------------------------------------\n",
            "Total Timesteps: 271493 Episode Num: 302 Reward: 582.4674266173481\n",
            "Total Timesteps: 272493 Episode Num: 303 Reward: 672.5305456358775\n",
            "Total Timesteps: 273493 Episode Num: 304 Reward: 314.3824757796308\n",
            "Total Timesteps: 274493 Episode Num: 305 Reward: 600.6342107201572\n",
            "Total Timesteps: 275493 Episode Num: 306 Reward: 523.6301170544522\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 702.633599\n",
            "---------------------------------------\n",
            "Total Timesteps: 276493 Episode Num: 307 Reward: 765.350612579733\n",
            "Total Timesteps: 277493 Episode Num: 308 Reward: 793.4277988598742\n",
            "Total Timesteps: 278493 Episode Num: 309 Reward: 698.953872328318\n",
            "Total Timesteps: 279493 Episode Num: 310 Reward: 717.0792504785526\n",
            "Total Timesteps: 280493 Episode Num: 311 Reward: 624.5386461171264\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 635.444178\n",
            "---------------------------------------\n",
            "Total Timesteps: 281493 Episode Num: 312 Reward: 548.7023381752468\n",
            "Total Timesteps: 282493 Episode Num: 313 Reward: 725.9525677274559\n",
            "Total Timesteps: 283493 Episode Num: 314 Reward: 548.6578177548796\n",
            "Total Timesteps: 284493 Episode Num: 315 Reward: 676.5812154208875\n",
            "Total Timesteps: 285493 Episode Num: 316 Reward: 681.3145379594665\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 678.966219\n",
            "---------------------------------------\n",
            "Total Timesteps: 286493 Episode Num: 317 Reward: 476.3339784820482\n",
            "Total Timesteps: 287493 Episode Num: 318 Reward: 794.8309029348186\n",
            "Total Timesteps: 288493 Episode Num: 319 Reward: 749.4282501480246\n",
            "Total Timesteps: 289493 Episode Num: 320 Reward: 554.4348480858695\n",
            "Total Timesteps: 290493 Episode Num: 321 Reward: 445.96805998091014\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 712.120526\n",
            "---------------------------------------\n",
            "Total Timesteps: 291493 Episode Num: 322 Reward: 697.6287798243645\n",
            "Total Timesteps: 292493 Episode Num: 323 Reward: 800.9009606811229\n",
            "Total Timesteps: 293493 Episode Num: 324 Reward: 760.810574214364\n",
            "Total Timesteps: 294493 Episode Num: 325 Reward: 756.0677792467559\n",
            "Total Timesteps: 295493 Episode Num: 326 Reward: 690.9859317076792\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 759.073574\n",
            "---------------------------------------\n",
            "Total Timesteps: 296493 Episode Num: 327 Reward: 621.6587280005749\n",
            "Total Timesteps: 297493 Episode Num: 328 Reward: 619.3788283065712\n",
            "Total Timesteps: 298493 Episode Num: 329 Reward: 645.1801560648876\n",
            "Total Timesteps: 299493 Episode Num: 330 Reward: 714.8820333865991\n",
            "Total Timesteps: 300493 Episode Num: 331 Reward: 660.8981109405296\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 702.805766\n",
            "---------------------------------------\n",
            "Total Timesteps: 301493 Episode Num: 332 Reward: 795.4259303599454\n",
            "Total Timesteps: 302493 Episode Num: 333 Reward: 853.1774828331893\n",
            "Total Timesteps: 303493 Episode Num: 334 Reward: 773.4848697674279\n",
            "Total Timesteps: 304493 Episode Num: 335 Reward: 868.1392736812694\n",
            "Total Timesteps: 305493 Episode Num: 336 Reward: 747.8699688312827\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 857.835434\n",
            "---------------------------------------\n",
            "Total Timesteps: 306493 Episode Num: 337 Reward: 791.5411885731257\n",
            "Total Timesteps: 307493 Episode Num: 338 Reward: 823.5903906625977\n",
            "Total Timesteps: 308493 Episode Num: 339 Reward: 629.4881944484789\n",
            "Total Timesteps: 309493 Episode Num: 340 Reward: 756.9654118532459\n",
            "Total Timesteps: 310493 Episode Num: 341 Reward: 658.0701735735505\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 804.045982\n",
            "---------------------------------------\n",
            "Total Timesteps: 311493 Episode Num: 342 Reward: 629.1371636555629\n",
            "Total Timesteps: 312493 Episode Num: 343 Reward: 770.9239181910642\n",
            "Total Timesteps: 313493 Episode Num: 344 Reward: 811.4453070879513\n",
            "Total Timesteps: 314493 Episode Num: 345 Reward: 855.7609930974775\n",
            "Total Timesteps: 315493 Episode Num: 346 Reward: 902.0440843365783\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 879.632692\n",
            "---------------------------------------\n",
            "Total Timesteps: 316493 Episode Num: 347 Reward: 905.5800641346157\n",
            "Total Timesteps: 317493 Episode Num: 348 Reward: 899.5655835866212\n",
            "Total Timesteps: 318493 Episode Num: 349 Reward: 702.2925575125925\n",
            "Total Timesteps: 319493 Episode Num: 350 Reward: 717.6128570663307\n",
            "Total Timesteps: 320493 Episode Num: 351 Reward: 647.358126943842\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 824.407102\n",
            "---------------------------------------\n",
            "Total Timesteps: 321493 Episode Num: 352 Reward: 752.4821662817928\n",
            "Total Timesteps: 322493 Episode Num: 353 Reward: 1010.0678518526955\n",
            "Total Timesteps: 323493 Episode Num: 354 Reward: 791.2286360391355\n",
            "Total Timesteps: 324493 Episode Num: 355 Reward: 700.3985593288112\n",
            "Total Timesteps: 325493 Episode Num: 356 Reward: 859.4153772677727\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 660.915390\n",
            "---------------------------------------\n",
            "Total Timesteps: 326493 Episode Num: 357 Reward: 818.6511072158776\n",
            "Total Timesteps: 327493 Episode Num: 358 Reward: 717.5871284588662\n",
            "Total Timesteps: 328493 Episode Num: 359 Reward: 995.2584406919798\n",
            "Total Timesteps: 329493 Episode Num: 360 Reward: 797.0489881704613\n",
            "Total Timesteps: 330493 Episode Num: 361 Reward: 857.3988680400292\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 949.780747\n",
            "---------------------------------------\n",
            "Total Timesteps: 331493 Episode Num: 362 Reward: 1053.5343804540928\n",
            "Total Timesteps: 332493 Episode Num: 363 Reward: 988.0680423842817\n",
            "Total Timesteps: 333493 Episode Num: 364 Reward: 868.6556609587667\n",
            "Total Timesteps: 334493 Episode Num: 365 Reward: 913.2189335007613\n",
            "Total Timesteps: 335493 Episode Num: 366 Reward: 724.1579647367596\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 742.850856\n",
            "---------------------------------------\n",
            "Total Timesteps: 336493 Episode Num: 367 Reward: 677.4593600922112\n",
            "Total Timesteps: 337493 Episode Num: 368 Reward: 716.937961206089\n",
            "Total Timesteps: 338493 Episode Num: 369 Reward: 673.0572064877922\n",
            "Total Timesteps: 339493 Episode Num: 370 Reward: 796.5110584285189\n",
            "Total Timesteps: 340493 Episode Num: 371 Reward: 581.8097150461343\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 693.132005\n",
            "---------------------------------------\n",
            "Total Timesteps: 341493 Episode Num: 372 Reward: 656.511059595489\n",
            "Total Timesteps: 342493 Episode Num: 373 Reward: 1035.3545278494828\n",
            "Total Timesteps: 343493 Episode Num: 374 Reward: 749.2362465639825\n",
            "Total Timesteps: 344493 Episode Num: 375 Reward: 865.7152997841221\n",
            "Total Timesteps: 345493 Episode Num: 376 Reward: 928.4581418331055\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 837.897743\n",
            "---------------------------------------\n",
            "Total Timesteps: 346493 Episode Num: 377 Reward: 814.2905449390025\n",
            "Total Timesteps: 347493 Episode Num: 378 Reward: 673.8150995700595\n",
            "Total Timesteps: 348493 Episode Num: 379 Reward: 946.4739351444578\n",
            "Total Timesteps: 349493 Episode Num: 380 Reward: 923.3026714771223\n",
            "Total Timesteps: 350493 Episode Num: 381 Reward: 982.9038383544049\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 857.516147\n",
            "---------------------------------------\n",
            "Total Timesteps: 351493 Episode Num: 382 Reward: 798.9771106369478\n",
            "Total Timesteps: 352493 Episode Num: 383 Reward: 659.9698771623064\n",
            "Total Timesteps: 353493 Episode Num: 384 Reward: 799.2321201755981\n",
            "Total Timesteps: 354493 Episode Num: 385 Reward: 693.1082311496358\n",
            "Total Timesteps: 355493 Episode Num: 386 Reward: 730.8310070921168\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 739.267427\n",
            "---------------------------------------\n",
            "Total Timesteps: 356493 Episode Num: 387 Reward: 560.4432508892803\n",
            "Total Timesteps: 357493 Episode Num: 388 Reward: 781.1598488223563\n",
            "Total Timesteps: 358493 Episode Num: 389 Reward: 577.1460014399793\n",
            "Total Timesteps: 359493 Episode Num: 390 Reward: 857.5844255230807\n",
            "Total Timesteps: 360493 Episode Num: 391 Reward: 840.8879328754937\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 757.872391\n",
            "---------------------------------------\n",
            "Total Timesteps: 361493 Episode Num: 392 Reward: 702.7248730471784\n",
            "Total Timesteps: 362493 Episode Num: 393 Reward: 828.2800359088443\n",
            "Total Timesteps: 363493 Episode Num: 394 Reward: 888.8699110846114\n",
            "Total Timesteps: 364493 Episode Num: 395 Reward: 1039.3785225367417\n",
            "Total Timesteps: 365493 Episode Num: 396 Reward: 789.8977359009784\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 987.342757\n",
            "---------------------------------------\n",
            "Total Timesteps: 366493 Episode Num: 397 Reward: 993.0523329244868\n",
            "Total Timesteps: 367493 Episode Num: 398 Reward: 1262.9591406700847\n",
            "Total Timesteps: 368493 Episode Num: 399 Reward: 1284.5180268105755\n",
            "Total Timesteps: 369493 Episode Num: 400 Reward: 1179.8713449707216\n",
            "Total Timesteps: 370493 Episode Num: 401 Reward: 1228.6273453192148\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1315.157689\n",
            "---------------------------------------\n",
            "Total Timesteps: 371493 Episode Num: 402 Reward: 1359.5976408608974\n",
            "Total Timesteps: 372493 Episode Num: 403 Reward: 1343.780216011258\n",
            "Total Timesteps: 373493 Episode Num: 404 Reward: 857.9097981849466\n",
            "Total Timesteps: 374493 Episode Num: 405 Reward: 1002.9745574323936\n",
            "Total Timesteps: 375493 Episode Num: 406 Reward: 951.9582074515891\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1393.626693\n",
            "---------------------------------------\n",
            "Total Timesteps: 376493 Episode Num: 407 Reward: 1387.4168723673822\n",
            "Total Timesteps: 377493 Episode Num: 408 Reward: 1423.4187127436658\n",
            "Total Timesteps: 378493 Episode Num: 409 Reward: 1499.7993782354438\n",
            "Total Timesteps: 379493 Episode Num: 410 Reward: 1561.4774479590944\n",
            "Total Timesteps: 380493 Episode Num: 411 Reward: 1532.3388495040545\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1244.490259\n",
            "---------------------------------------\n",
            "Total Timesteps: 381493 Episode Num: 412 Reward: 1306.747509062216\n",
            "Total Timesteps: 382493 Episode Num: 413 Reward: 1184.296273047554\n",
            "Total Timesteps: 383493 Episode Num: 414 Reward: 1310.646384364663\n",
            "Total Timesteps: 384493 Episode Num: 415 Reward: 1152.1027062892215\n",
            "Total Timesteps: 385493 Episode Num: 416 Reward: 1140.393706512502\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1315.796564\n",
            "---------------------------------------\n",
            "Total Timesteps: 386493 Episode Num: 417 Reward: 1403.1357444308103\n",
            "Total Timesteps: 387493 Episode Num: 418 Reward: 1446.522761443167\n",
            "Total Timesteps: 388493 Episode Num: 419 Reward: 1391.756164922674\n",
            "Total Timesteps: 389493 Episode Num: 420 Reward: 1490.8035172745463\n",
            "Total Timesteps: 390493 Episode Num: 421 Reward: 1368.1075719918213\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1475.763850\n",
            "---------------------------------------\n",
            "Total Timesteps: 391493 Episode Num: 422 Reward: 1419.3656888278597\n",
            "Total Timesteps: 392493 Episode Num: 423 Reward: 1487.2366254184142\n",
            "Total Timesteps: 393493 Episode Num: 424 Reward: 1150.7324378602868\n",
            "Total Timesteps: 394493 Episode Num: 425 Reward: 1231.6835707319606\n",
            "Total Timesteps: 395493 Episode Num: 426 Reward: 1492.3633507370878\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1091.504252\n",
            "---------------------------------------\n",
            "Total Timesteps: 396493 Episode Num: 427 Reward: 805.0246817363872\n",
            "Total Timesteps: 397493 Episode Num: 428 Reward: 1262.500359301431\n",
            "Total Timesteps: 398493 Episode Num: 429 Reward: 1511.4609998314968\n",
            "Total Timesteps: 399493 Episode Num: 430 Reward: 1220.6615948504932\n",
            "Total Timesteps: 400493 Episode Num: 431 Reward: 1618.4519435046495\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1639.271056\n",
            "---------------------------------------\n",
            "Total Timesteps: 401493 Episode Num: 432 Reward: 1558.9483781329689\n",
            "Total Timesteps: 402493 Episode Num: 433 Reward: 1403.4008819423116\n",
            "Total Timesteps: 403493 Episode Num: 434 Reward: 1533.681186626018\n",
            "Total Timesteps: 404493 Episode Num: 435 Reward: 1180.7643753883624\n",
            "Total Timesteps: 405493 Episode Num: 436 Reward: 1530.5497744912905\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1530.630980\n",
            "---------------------------------------\n",
            "Total Timesteps: 406493 Episode Num: 437 Reward: 1542.8174889969794\n",
            "Total Timesteps: 407493 Episode Num: 438 Reward: 1566.2899922556721\n",
            "Total Timesteps: 408493 Episode Num: 439 Reward: 1480.5775017351177\n",
            "Total Timesteps: 409493 Episode Num: 440 Reward: 1550.2119648844136\n",
            "Total Timesteps: 410493 Episode Num: 441 Reward: 1328.9345892967156\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 732.142299\n",
            "---------------------------------------\n",
            "Total Timesteps: 411493 Episode Num: 442 Reward: 771.6820053516388\n",
            "Total Timesteps: 412493 Episode Num: 443 Reward: 1040.1178889927992\n",
            "Total Timesteps: 413493 Episode Num: 444 Reward: 1064.868655440589\n",
            "Total Timesteps: 414493 Episode Num: 445 Reward: 1223.036187165516\n",
            "Total Timesteps: 415493 Episode Num: 446 Reward: 1609.2024364425693\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1362.439093\n",
            "---------------------------------------\n",
            "Total Timesteps: 416493 Episode Num: 447 Reward: 1317.8314436831865\n",
            "Total Timesteps: 417493 Episode Num: 448 Reward: 995.1514662704096\n",
            "Total Timesteps: 418493 Episode Num: 449 Reward: 880.9861315713512\n",
            "Total Timesteps: 419493 Episode Num: 450 Reward: 1269.5033608140463\n",
            "Total Timesteps: 420493 Episode Num: 451 Reward: 1060.3591310298325\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 703.923985\n",
            "---------------------------------------\n",
            "Total Timesteps: 421493 Episode Num: 452 Reward: 731.2860428894326\n",
            "Total Timesteps: 422493 Episode Num: 453 Reward: 664.161994465263\n",
            "Total Timesteps: 423493 Episode Num: 454 Reward: 837.2662514666232\n",
            "Total Timesteps: 424493 Episode Num: 455 Reward: 1554.8609110629243\n",
            "Total Timesteps: 425493 Episode Num: 456 Reward: 1627.818386476682\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1586.515369\n",
            "---------------------------------------\n",
            "Total Timesteps: 426493 Episode Num: 457 Reward: 1519.9008580692152\n",
            "Total Timesteps: 427493 Episode Num: 458 Reward: 521.7651580922009\n",
            "Total Timesteps: 428493 Episode Num: 459 Reward: 1002.6107283076963\n",
            "Total Timesteps: 429493 Episode Num: 460 Reward: 1738.8870258786549\n",
            "Total Timesteps: 430493 Episode Num: 461 Reward: 1512.4575227198181\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1754.589236\n",
            "---------------------------------------\n",
            "Total Timesteps: 431493 Episode Num: 462 Reward: 1684.0521221649649\n",
            "Total Timesteps: 432493 Episode Num: 463 Reward: 1479.3708901534296\n",
            "Total Timesteps: 433493 Episode Num: 464 Reward: 1210.8586067191832\n",
            "Total Timesteps: 434493 Episode Num: 465 Reward: 1666.1047667389557\n",
            "Total Timesteps: 435493 Episode Num: 466 Reward: 1646.9750912937197\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1747.361153\n",
            "---------------------------------------\n",
            "Total Timesteps: 436493 Episode Num: 467 Reward: 1709.9909513317752\n",
            "Total Timesteps: 437493 Episode Num: 468 Reward: 1594.246234953255\n",
            "Total Timesteps: 438493 Episode Num: 469 Reward: 1703.6800228904858\n",
            "Total Timesteps: 439493 Episode Num: 470 Reward: 1736.2329100835145\n",
            "Total Timesteps: 440493 Episode Num: 471 Reward: 1807.1432019059841\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1703.487660\n",
            "---------------------------------------\n",
            "Total Timesteps: 441493 Episode Num: 472 Reward: 1653.1227265578602\n",
            "Total Timesteps: 442493 Episode Num: 473 Reward: 1737.4679180322812\n",
            "Total Timesteps: 443493 Episode Num: 474 Reward: 1714.3544620705875\n",
            "Total Timesteps: 444493 Episode Num: 475 Reward: 1803.09272619831\n",
            "Total Timesteps: 445493 Episode Num: 476 Reward: 1766.3215063559267\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1797.195258\n",
            "---------------------------------------\n",
            "Total Timesteps: 446493 Episode Num: 477 Reward: 1764.2240590554072\n",
            "Total Timesteps: 447493 Episode Num: 478 Reward: 1704.0556826726129\n",
            "Total Timesteps: 448493 Episode Num: 479 Reward: 1668.5315135567675\n",
            "Total Timesteps: 449493 Episode Num: 480 Reward: 1802.2192778666713\n",
            "Total Timesteps: 450493 Episode Num: 481 Reward: 1584.1471625408112\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1740.772974\n",
            "---------------------------------------\n",
            "Total Timesteps: 451493 Episode Num: 482 Reward: 1746.712088744841\n",
            "Total Timesteps: 452493 Episode Num: 483 Reward: 1783.5566707441565\n",
            "Total Timesteps: 453493 Episode Num: 484 Reward: 1597.4999443179386\n",
            "Total Timesteps: 454493 Episode Num: 485 Reward: 1740.960423466237\n",
            "Total Timesteps: 455493 Episode Num: 486 Reward: 1711.1484597645647\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1535.637820\n",
            "---------------------------------------\n",
            "Total Timesteps: 456493 Episode Num: 487 Reward: 1450.4106307673906\n",
            "Total Timesteps: 457493 Episode Num: 488 Reward: 1537.6657291893027\n",
            "Total Timesteps: 458493 Episode Num: 489 Reward: 1650.307156744691\n",
            "Total Timesteps: 459493 Episode Num: 490 Reward: 1645.4828196049211\n",
            "Total Timesteps: 460493 Episode Num: 491 Reward: 1323.3424341136451\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1505.869580\n",
            "---------------------------------------\n",
            "Total Timesteps: 461493 Episode Num: 492 Reward: 1469.7883839369101\n",
            "Total Timesteps: 462493 Episode Num: 493 Reward: 1472.1633929792736\n",
            "Total Timesteps: 463493 Episode Num: 494 Reward: 1593.2814524987746\n",
            "Total Timesteps: 464493 Episode Num: 495 Reward: 1587.1613952717075\n",
            "Total Timesteps: 465493 Episode Num: 496 Reward: 1444.1675246253726\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1518.807347\n",
            "---------------------------------------\n",
            "Total Timesteps: 466493 Episode Num: 497 Reward: 1444.6308990024947\n",
            "Total Timesteps: 467493 Episode Num: 498 Reward: 1648.9318955078732\n",
            "Total Timesteps: 468493 Episode Num: 499 Reward: 1568.5118968251984\n",
            "Total Timesteps: 469493 Episode Num: 500 Reward: 1680.2203142037788\n",
            "Total Timesteps: 470493 Episode Num: 501 Reward: 1871.3183137925305\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1843.279722\n",
            "---------------------------------------\n",
            "Total Timesteps: 471493 Episode Num: 502 Reward: 1824.723328160192\n",
            "Total Timesteps: 472493 Episode Num: 503 Reward: 1855.5427715024985\n",
            "Total Timesteps: 473493 Episode Num: 504 Reward: 1709.8938977222842\n",
            "Total Timesteps: 474493 Episode Num: 505 Reward: 1922.3398668199077\n",
            "Total Timesteps: 475493 Episode Num: 506 Reward: 1746.9999797588328\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1929.733841\n",
            "---------------------------------------\n",
            "Total Timesteps: 476493 Episode Num: 507 Reward: 1916.030303604608\n",
            "Total Timesteps: 477493 Episode Num: 508 Reward: 2005.3978882167762\n",
            "Total Timesteps: 478493 Episode Num: 509 Reward: 1769.826983409434\n",
            "Total Timesteps: 479493 Episode Num: 510 Reward: 1966.1077710221762\n",
            "Total Timesteps: 480493 Episode Num: 511 Reward: 1871.3987629171804\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1874.182299\n",
            "---------------------------------------\n",
            "Total Timesteps: 481493 Episode Num: 512 Reward: 1862.246161215329\n",
            "Total Timesteps: 482493 Episode Num: 513 Reward: 1922.5159744599669\n",
            "Total Timesteps: 483493 Episode Num: 514 Reward: 1937.5243475982672\n",
            "Total Timesteps: 484493 Episode Num: 515 Reward: 1992.4823141006616\n",
            "Total Timesteps: 485493 Episode Num: 516 Reward: 1662.9501400291008\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1998.615152\n",
            "---------------------------------------\n",
            "Total Timesteps: 486493 Episode Num: 517 Reward: 1949.2699340300358\n",
            "Total Timesteps: 487493 Episode Num: 518 Reward: 1828.9939544268032\n",
            "Total Timesteps: 488493 Episode Num: 519 Reward: 1936.4562345136853\n",
            "Total Timesteps: 489493 Episode Num: 520 Reward: 1873.2687083772862\n",
            "Total Timesteps: 490493 Episode Num: 521 Reward: 1952.253493746057\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1982.877352\n",
            "---------------------------------------\n",
            "Total Timesteps: 491493 Episode Num: 522 Reward: 1847.5542753119246\n",
            "Total Timesteps: 492493 Episode Num: 523 Reward: 1872.2169210587429\n",
            "Total Timesteps: 493493 Episode Num: 524 Reward: 1894.2039560663018\n",
            "Total Timesteps: 494493 Episode Num: 525 Reward: 1896.5057973968294\n",
            "Total Timesteps: 495493 Episode Num: 526 Reward: 1936.2075361234467\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1954.230154\n",
            "---------------------------------------\n",
            "Total Timesteps: 496493 Episode Num: 527 Reward: 1943.5153073266697\n",
            "Total Timesteps: 497493 Episode Num: 528 Reward: 1794.3138032648899\n",
            "Total Timesteps: 498493 Episode Num: 529 Reward: 1765.5579319262117\n",
            "Total Timesteps: 499493 Episode Num: 530 Reward: 1916.25407606476\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1928.802543\n",
            "---------------------------------------\n"
          ]
        }
      ],
      "source": [
        "max_timesteps = 500000\n",
        "# We start the main loop over 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "\n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "\n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "\n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "\n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "\n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "\n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "  # We check if the episode is done\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "\n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "\n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi6e2-_pu05e"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oW4d1YAMqif1",
        "outputId": "6ae29865-3bdd-4f74-a6cf-45bb4741b718"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1935.127511\n",
            "---------------------------------------\n"
          ]
        }
      ],
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1\n",
        "\n",
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "\n",
        "    for it in range(iterations):\n",
        "\n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "\n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "\n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "\n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "\n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "\n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "\n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "\n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward\n",
        "\n",
        "env_name = \"AntBulletEnv-v0\"\n",
        "seed = 0\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "eval_episodes = 10\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "max_episode_steps = env._max_episode_steps\n",
        "if save_env_vid:\n",
        "  # env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env = gym.wrappers.Monitor(env, monitor_dir, force=True)\n",
        "  env.reset()\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
